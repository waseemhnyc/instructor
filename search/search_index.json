{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor","text":"<p>Structured extraction in Python, powered by OpenAI's function calling api, designed for simplicity, transparency, and control.</p> <p> </p> <p>Dive into the world of Python-based structured extraction, by OpenAI's function calling API and Pydantic, the most widely used data validation library for Python. Instructor stands out for its simplicity, transparency, and user-centric design. Whether you're a seasoned developer or just starting out, you'll find Instructor's approach intuitive and steerable.</p>"},{"location":"#usage","title":"Usage","text":"<pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n)\n\nassert isinstance(user, UserDetail)\nassert user.name == \"Jason\"\nassert user.age == 25\n</code></pre> <p>Using async clients</p> <p>For async clients you must use apatch vs patch like so:</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\naclient = instructor.apatch(AsyncOpenAI())\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nmodel = await aclient.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(model, UserExtract)\n</code></pre> <p>Accessing the original response</p> <p>If you want to access anything like usage or other metadata, the original response is available on the <code>Model._raw_response</code> attribute.</p> <pre><code>user: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n)\n\nfrom openai.types.chat.chat_completion import ChatCompletion\n\nassert isinstance(user._raw_response, ChatCompletion)\n</code></pre>"},{"location":"#why-use-instructor","title":"Why use Instructor?","text":"<p>The question of using Instructor is fundamentally a question of why to use Pydantic.</p> <ol> <li> <p>Powered by type hints \u2014 Instructor is powered by Pydantic, which is powered by type hints. Schema validation, prompting is controleld by type annotations; less to learn, less code ot write, and integrates with your IDE.</p> </li> <li> <p>Powered by OpenAI \u2014 Instructor is powered by OpenAI's function calling API. This means you can use the same API for both prompting and extraction.</p> </li> <li> <p>Customizable \u2014 Pydantic is highly customizable. You can define your own validators, custom error messages, and more.</p> </li> <li> <p>Ecosystem Pydantic is the most widely used data validation library for Python. It's used by FastAPI, Typer, and many other popular libraries.</p> </li> <li> <p>Battle Tested \u2014 Pydantic is downloaded over 100M times per month, and supported by a large community of contributors.</p> </li> <li> <p>Easy Integration with CLI - We offer a variety of CLI tools like <code>instructor jobs</code>, <code>instructor files</code> and <code>instructor usage</code> to track your OpenAI usage, fine-tuning jobs and more, just check out our CLI Documentation to find out more.</p> </li> </ol>"},{"location":"#more-examples","title":"More Examples","text":"<p>If you'd like to see more check out our cookbook.</p> <p>Installing Instructor is a breeze. Just run <code>pip install instructor</code>.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you want to help out checkout some of the issues marked as <code>good-first-issue</code> or <code>help-wanted</code>. Found here. They could be anything from code improvements, a guest blog post, or a new cook book.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#instructor.patch.apatch","title":"<code>apatch(client, mode=Mode.FUNCTIONS)</code>","text":"<p>No longer necessary, use <code>patch</code> instead.</p> <p>Patch the <code>client.chat.completions.create</code> method</p> <p>Enables the following features:</p> <ul> <li><code>response_model</code> parameter to parse the response from OpenAI's API</li> <li><code>max_retries</code> parameter to retry the function if the response is not valid</li> <li><code>validation_context</code> parameter to validate the response using the pydantic model</li> <li><code>strict</code> parameter to use strict json parsing</li> </ul> Source code in <code>instructor/patch.py</code> <pre><code>def apatch(client: AsyncOpenAI, mode: Mode = Mode.FUNCTIONS):\n    \"\"\"\n    No longer necessary, use `patch` instead.\n\n    Patch the `client.chat.completions.create` method\n\n    Enables the following features:\n\n    - `response_model` parameter to parse the response from OpenAI's API\n    - `max_retries` parameter to retry the function if the response is not valid\n    - `validation_context` parameter to validate the response using the pydantic model\n    - `strict` parameter to use strict json parsing\n    \"\"\"\n    return patch(client, mode=mode)\n</code></pre>"},{"location":"api/#instructor.patch.dump_message","title":"<code>dump_message(message)</code>","text":"<p>Dumps a message to a dict, to be returned to the OpenAI API. Workaround for an issue with the OpenAI API, where the <code>tool_calls</code> field isn't allowed to be present in requests if it isn't used.</p> Source code in <code>instructor/patch.py</code> <pre><code>def dump_message(message) -&gt; dict:\n    \"\"\"Dumps a message to a dict, to be returned to the OpenAI API.\n    Workaround for an issue with the OpenAI API, where the `tool_calls` field isn't allowed to be present in requests\n    if it isn't used.\n    \"\"\"\n    dumped_message = message.model_dump()\n    if not dumped_message.get(\"tool_calls\"):\n        del dumped_message[\"tool_calls\"]\n    return {k: v for k, v in dumped_message.items() if v}\n</code></pre>"},{"location":"api/#instructor.patch.is_async","title":"<code>is_async(func)</code>","text":"<p>Returns true if the callable is async, accounting for wrapped callables</p> Source code in <code>instructor/patch.py</code> <pre><code>def is_async(func: Callable) -&gt; bool:\n    \"\"\"Returns true if the callable is async, accounting for wrapped callables\"\"\"\n    return inspect.iscoroutinefunction(func) or (\n        hasattr(func, \"__wrapped__\") and inspect.iscoroutinefunction(func.__wrapped__)\n    )\n</code></pre>"},{"location":"api/#instructor.patch.patch","title":"<code>patch(client, mode=Mode.FUNCTIONS)</code>","text":"<p>Patch the <code>client.chat.completions.create</code> method</p> <p>Enables the following features:</p> <ul> <li><code>response_model</code> parameter to parse the response from OpenAI's API</li> <li><code>max_retries</code> parameter to retry the function if the response is not valid</li> <li><code>validation_context</code> parameter to validate the response using the pydantic model</li> <li><code>strict</code> parameter to use strict json parsing</li> </ul> Source code in <code>instructor/patch.py</code> <pre><code>def patch(client: Union[OpenAI, AsyncOpenAI], mode: Mode = Mode.FUNCTIONS):\n    \"\"\"\n    Patch the `client.chat.completions.create` method\n\n    Enables the following features:\n\n    - `response_model` parameter to parse the response from OpenAI's API\n    - `max_retries` parameter to retry the function if the response is not valid\n    - `validation_context` parameter to validate the response using the pydantic model\n    - `strict` parameter to use strict json parsing\n    \"\"\"\n\n    client.chat.completions.create = wrap_chatcompletion(\n        client.chat.completions.create, mode=mode\n    )\n    return client\n</code></pre>"},{"location":"api/#instructor.patch.process_response","title":"<code>process_response(response, *, response_model, stream, validation_context=None, strict=None, mode=Mode.FUNCTIONS)</code>","text":"<p>Processes a OpenAI response with the response model, if available. It can use <code>validation_context</code> and <code>strict</code> to validate the response via the pydantic model</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatCompletion</code> <p>The response from OpenAI's API</p> required <code>response_model</code> <code>BaseModel</code> <p>The response model to use for parsing the response</p> required <code>stream</code> <code>bool</code> <p>Whether the response is a stream</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response. Defaults to None.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing. Defaults to None.</p> <code>None</code> Source code in <code>instructor/patch.py</code> <pre><code>def process_response(\n    response,\n    *,\n    response_model: Type[BaseModel],\n    stream: bool,\n    validation_context: dict = None,\n    strict=None,\n    mode: Mode = Mode.FUNCTIONS,\n):  # type: ignore\n    \"\"\"Processes a OpenAI response with the response model, if available.\n    It can use `validation_context` and `strict` to validate the response\n    via the pydantic model\n\n    Args:\n        response (ChatCompletion): The response from OpenAI's API\n        response_model (BaseModel): The response model to use for parsing the response\n        stream (bool): Whether the response is a stream\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (bool, optional): Whether to use strict json parsing. Defaults to None.\n    \"\"\"\n    if response_model is not None:\n        is_model_multitask = issubclass(response_model, MultiTaskBase)\n        model = response_model.from_response(\n            response,\n            validation_context=validation_context,\n            strict=strict,\n            mode=mode,\n            stream_multitask=stream and is_model_multitask,\n        )\n        if not stream:\n            model._raw_response = response\n            if is_model_multitask:\n                return model.tasks\n        return model\n    return response\n</code></pre>"},{"location":"api/#instructor.dsl.validators.Validator","title":"<code>Validator</code>","text":"<p>             Bases: <code>OpenAISchema</code></p> <p>Validate if an attribute is correct and if not, return a new value with an error message</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>class Validator(OpenAISchema):\n    \"\"\"\n    Validate if an attribute is correct and if not,\n    return a new value with an error message\n    \"\"\"\n\n    is_valid: bool = Field(\n        default=True,\n        description=\"Whether the attribute is valid based on the requirements\",\n    )\n    reason: Optional[str] = Field(\n        default=None,\n        description=\"The error message if the attribute is not valid, otherwise None\",\n    )\n    fixed_value: Optional[str] = Field(\n        default=None,\n        description=\"If the attribute is not valid, suggest a new value for the attribute\",\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.validators.llm_validator","title":"<code>llm_validator(statement, allow_override=False, model='gpt-3.5-turbo', temperature=0, openai_client=None)</code>","text":"<p>Create a validator that uses the LLM to validate an attribute</p>"},{"location":"api/#instructor.dsl.validators.llm_validator--usage","title":"Usage","text":"<pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, Field, field_validator\n\nclass User(BaseModel):\n    name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n    age: int = Field(description=\"The age of the person\")\n\ntry:\n    user = User(name=\"Jason Liu\", age=20)\nexcept ValidationError as e:\n    print(e)\n</code></pre> <pre><code>1 validation error for User\nname\n  The name is valid but not all lowercase (type=value_error.llm_validator)\n</code></pre> <p>Note that there, the error message is written by the LLM, and the error type is <code>value_error.llm_validator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The statement to validate</p> required <code>model</code> <code>str</code> <p>The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")</p> <code>'gpt-3.5-turbo'</code> <code>temperature</code> <code>float</code> <p>The temperature to use for the LLM (default: 0)</p> <code>0</code> <code>openai_client</code> <code>OpenAI</code> <p>The OpenAI client to use (default: None)</p> <code>None</code> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def llm_validator(\n    statement: str,\n    allow_override: bool = False,\n    model: str = \"gpt-3.5-turbo\",\n    temperature: float = 0,\n    openai_client: OpenAI = None,\n):\n    \"\"\"\n    Create a validator that uses the LLM to validate an attribute\n\n    ## Usage\n\n    ```python\n    from instructor import llm_validator\n    from pydantic import BaseModel, Field, field_validator\n\n    class User(BaseModel):\n        name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n        age: int = Field(description=\"The age of the person\")\n\n    try:\n        user = User(name=\"Jason Liu\", age=20)\n    except ValidationError as e:\n        print(e)\n    ```\n\n    ```\n    1 validation error for User\n    name\n      The name is valid but not all lowercase (type=value_error.llm_validator)\n    ```\n\n    Note that there, the error message is written by the LLM, and the error type is `value_error.llm_validator`.\n\n    Parameters:\n        statement (str): The statement to validate\n        model (str): The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")\n        temperature (float): The temperature to use for the LLM (default: 0)\n        openai_client (OpenAI): The OpenAI client to use (default: None)\n    \"\"\"\n\n    openai_client = openai_client or OpenAI()\n\n    def llm(v):\n        resp = openai_client.chat.completions.create(\n            functions=[Validator.openai_schema],\n            function_call={\"name\": Validator.openai_schema[\"name\"]},\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does `{v}` follow the rules: {statement}\",\n                },\n            ],\n            model=model,\n            temperature=temperature,\n        )  # type: ignore\n        resp = Validator.from_response(resp)\n\n        # If the response is  not valid, return the reason, this could be used in\n        # the future to generate a better response, via reasking mechanism.\n        assert resp.is_valid, resp.reason\n\n        if allow_override and not resp.is_valid and resp.fixed_value is not None:\n            # If the value is not valid, but we allow override, return the fixed value\n            return resp.fixed_value\n        return v\n\n    return llm\n</code></pre>"},{"location":"api/#instructor.dsl.validators.openai_moderation","title":"<code>openai_moderation(client=None)</code>","text":"<p>Validates a message using OpenAI moderation model.</p> <p>Should only be used for monitoring inputs and outputs of OpenAI APIs Other use cases are disallowed as per: https://platform.openai.com/docs/guides/moderation/overview</p> <p>Example: <pre><code>from instructor import OpenAIModeration\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\nResponse(message=\"I hate you\")\n</code></pre></p> <pre><code> ValidationError: 1 validation error for Response\n message\nValue error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n</code></pre> <p>client (OpenAI): The OpenAI client to use, must be sync (default: None)</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def openai_moderation(client: OpenAI = None):\n    \"\"\"\n    Validates a message using OpenAI moderation model.\n\n    Should only be used for monitoring inputs and outputs of OpenAI APIs\n    Other use cases are disallowed as per:\n    https://platform.openai.com/docs/guides/moderation/overview\n\n    Example:\n    ```python\n    from instructor import OpenAIModeration\n\n    class Response(BaseModel):\n        message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\n    Response(message=\"I hate you\")\n    ```\n\n    ```\n     ValidationError: 1 validation error for Response\n     message\n    Value error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n    ```\n\n    client (OpenAI): The OpenAI client to use, must be sync (default: None)\n    \"\"\"\n\n    client = client or OpenAI()\n\n    def validate_message_with_openai_mod(v: str) -&gt; str:\n        response = client.moderations.create(input=v)\n        out = response.results[0]\n        cats = out.categories.model_dump()\n        if out.flagged:\n            raise ValueError(\n                f\"`{v}` was flagged for {', '.join(cat for cat in cats if cats[cat])}\"\n            )\n\n        return v\n\n    return validate_message_with_openai_mod\n</code></pre>"},{"location":"api/#instructor.dsl.citation.CitationMixin","title":"<code>CitationMixin</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Helpful mixing that can use <code>validation_context={\"context\": context}</code> in <code>from_response</code> to find the span of the substring_phrase in the context.</p>"},{"location":"api/#instructor.dsl.citation.CitationMixin--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import CitationMixin\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\n\ncontext = \"Betty was a student. Jason was a student. Jason is 20 years old\"\n\nuser = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract jason from {context}\",\n        },\n    response_model=User,\n    validation_context={\"context\": context},\n    ]\n)\n\nfor quote in user.substring_quotes:\n    assert quote in context\n\nprint(user.model_dump())\n</code></pre>"},{"location":"api/#instructor.dsl.citation.CitationMixin--result","title":"Result","text":"<pre><code>{\n    \"name\": \"Jason Liu\",\n    \"age\": 20,\n    \"role\": \"student\",\n    \"substring_quotes\": [\n        \"Jason was a student\",\n        \"Jason is 20 years old\",\n    ]\n}\n</code></pre> Source code in <code>instructor/dsl/citation.py</code> <pre><code>class CitationMixin(BaseModel):\n    \"\"\"\n    Helpful mixing that can use `validation_context={\"context\": context}` in `from_response` to find the span of the substring_phrase in the context.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import CitationMixin\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n\n    context = \"Betty was a student. Jason was a student. Jason is 20 years old\"\n\n    user = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract jason from {context}\",\n            },\n        response_model=User,\n        validation_context={\"context\": context},\n        ]\n    )\n\n    for quote in user.substring_quotes:\n        assert quote in context\n\n    print(user.model_dump())\n    ```\n\n    ## Result\n    ```\n    {\n        \"name\": \"Jason Liu\",\n        \"age\": 20,\n        \"role\": \"student\",\n        \"substring_quotes\": [\n            \"Jason was a student\",\n            \"Jason is 20 years old\",\n        ]\n    }\n    ```\n\n    \"\"\"\n\n    substring_quotes: List[str] = Field(\n        description=\"List of unique and specific substrings of the quote that was used to answer the question.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: FieldValidationInfo) -&gt; \"CitationMixin\":\n        \"\"\"\n        For each substring_phrase, find the span of the substring_phrase in the context.\n        If the span is not found, remove the substring_phrase from the list.\n        \"\"\"\n        if info.context is None:\n            return self\n\n        # Get the context from the info\n        text_chunks = info.context.get(\"context\", None)\n\n        # Get the spans of the substring_phrase in the context\n        spans = list(self.get_spans(text_chunks))\n        # Replace the substring_phrase with the actual substring\n        self.substring_quotes = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def _get_span(self, quote, context, errs=5):\n        import regex\n\n        minor = quote\n        major = context\n\n        errs_ = 0\n        s = regex.search(f\"({minor}){{e&lt;={errs_}}}\", major)\n        while s is None and errs_ &lt;= errs:\n            errs_ += 1\n            s = regex.search(f\"({minor}){{e&lt;={errs_}}}\", major)\n\n        if s is not None:\n            yield from s.spans()\n\n    def get_spans(self, context):\n        for quote in self.substring_quotes:\n            yield from self._get_span(quote, context)\n</code></pre>"},{"location":"api/#instructor.dsl.citation.CitationMixin.validate_sources","title":"<code>validate_sources(info)</code>","text":"<p>For each substring_phrase, find the span of the substring_phrase in the context. If the span is not found, remove the substring_phrase from the list.</p> Source code in <code>instructor/dsl/citation.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_sources(self, info: FieldValidationInfo) -&gt; \"CitationMixin\":\n    \"\"\"\n    For each substring_phrase, find the span of the substring_phrase in the context.\n    If the span is not found, remove the substring_phrase from the list.\n    \"\"\"\n    if info.context is None:\n        return self\n\n    # Get the context from the info\n    text_chunks = info.context.get(\"context\", None)\n\n    # Get the spans of the substring_phrase in the context\n    spans = list(self.get_spans(text_chunks))\n    # Replace the substring_phrase with the actual substring\n    self.substring_quotes = [text_chunks[span[0] : span[1]] for span in spans]\n    return self\n</code></pre>"},{"location":"api/#instructor.dsl.multitask.MultiTask","title":"<code>MultiTask(subtask_class, name=None, description=None)</code>","text":"<p>Dynamically create a MultiTask OpenAISchema that can be used to segment multiple tasks given a base class. This creates class that can be used to create a toolkit for a specific task, names and descriptions are automatically generated. However they can be overridden.</p>"},{"location":"api/#instructor.dsl.multitask.MultiTask--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import MultiTask\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMultiUser = MultiTask(User)\n</code></pre>"},{"location":"api/#instructor.dsl.multitask.MultiTask--result","title":"Result","text":"<pre><code>class MultiUser(OpenAISchema, MultiTaskBase):\n    tasks: List[User] = Field(\n        default_factory=list,\n        repr=False,\n        description=\"Correctly segmented list of `User` tasks\",\n    )\n\n    @classmethod\n    def from_streaming_response(cls, completion) -&gt; Generator[User]:\n        '''\n        Parse the streaming response from OpenAI and yield a `User` object\n        for each task in the response\n        '''\n        json_chunks = cls.extract_json(completion)\n        yield from cls.tasks_from_chunks(json_chunks)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subtask_class</code> <code>Type[OpenAISchema]</code> <p>The base class to use for the MultiTask</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the MultiTask class, if None then the name of the subtask class is used as <code>Multi{subtask_class.__name__}</code></p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the MultiTask class, if None then the description is set to <code>Correct segmentation of</code>{subtask_class.name}<code>tasks</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>schema</code> <code>OpenAISchema</code> <p>A new class that can be used to segment multiple tasks</p> Source code in <code>instructor/dsl/multitask.py</code> <pre><code>def MultiTask(\n    subtask_class: Type[BaseModel],\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n):\n    \"\"\"\n    Dynamically create a MultiTask OpenAISchema that can be used to segment multiple\n    tasks given a base class. This creates class that can be used to create a toolkit\n    for a specific task, names and descriptions are automatically generated. However\n    they can be overridden.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import MultiTask\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MultiUser = MultiTask(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MultiUser(OpenAISchema, MultiTaskBase):\n        tasks: List[User] = Field(\n            default_factory=list,\n            repr=False,\n            description=\"Correctly segmented list of `User` tasks\",\n        )\n\n        @classmethod\n        def from_streaming_response(cls, completion) -&gt; Generator[User]:\n            '''\n            Parse the streaming response from OpenAI and yield a `User` object\n            for each task in the response\n            '''\n            json_chunks = cls.extract_json(completion)\n            yield from cls.tasks_from_chunks(json_chunks)\n    ```\n\n    Parameters:\n        subtask_class (Type[OpenAISchema]): The base class to use for the MultiTask\n        name (Optional[str]): The name of the MultiTask class, if None then the name\n            of the subtask class is used as `Multi{subtask_class.__name__}`\n        description (Optional[str]): The description of the MultiTask class, if None\n            then the description is set to `Correct segmentation of `{subtask_class.__name__}` tasks`\n\n    Returns:\n        schema (OpenAISchema): A new class that can be used to segment multiple tasks\n    \"\"\"\n    task_name = subtask_class.__name__ if name is None else name\n\n    name = f\"Multi{task_name}\"\n\n    list_tasks = (\n        List[subtask_class],\n        Field(\n            default_factory=list,\n            repr=False,\n            description=f\"Correctly segmented list of `{task_name}` tasks\",\n        ),\n    )\n\n    new_cls = create_model(\n        name,\n        tasks=list_tasks,\n        __base__=(OpenAISchema, MultiTaskBase),  # type: ignore\n    )\n    # set the class constructor BaseModel\n    new_cls.task_type = subtask_class\n\n    new_cls.__doc__ = (\n        f\"Correct segmentation of `{task_name}` tasks\"\n        if description is None\n        else description\n    )\n\n    return new_cls\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.MaybeBase","title":"<code>MaybeBase</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Extract a result from a model, if any, otherwise set the error and message fields.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>class MaybeBase(BaseModel):\n    \"\"\"\n    Extract a result from a model, if any, otherwise set the error and message fields.\n    \"\"\"\n\n    result: Optional[BaseModel]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None  # type: ignore\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe","title":"<code>Maybe(model)</code>","text":"<p>Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code> for sitatations where the data may not be present in the context.</p>"},{"location":"api/#instructor.dsl.maybe.Maybe--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import Maybe\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMaybeUser = Maybe(User)\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe--result","title":"Result","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[User]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to wrap with Maybe.</p> required <p>Returns:</p> Name Type Description <code>MaybeModel</code> <code>Type[BaseModel]</code> <p>A new Pydantic model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code>.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>def Maybe(model: Type[BaseModel]) -&gt; MaybeBase:\n    \"\"\"\n    Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for `result`, `error`, and `message` for sitatations where the data may not be present in the context.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import Maybe\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MaybeUser = Maybe(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MaybeUser(BaseModel):\n        result: Optional[User]\n        error: bool = Field(default=False)\n        message: Optional[str]\n\n        def __bool__(self):\n            return self.result is not None\n    ```\n\n    Parameters:\n        model (Type[BaseModel]): The Pydantic model to wrap with Maybe.\n\n    Returns:\n        MaybeModel (Type[BaseModel]): A new Pydantic model that includes fields for `result`, `error`, and `message`.\n    \"\"\"\n\n    class MaybeBase(BaseModel):\n        def __bool__(self):\n            return self.result is not None  # type: ignore\n\n    fields = {\n        \"result\": (\n            Optional[model],\n            Field(\n                default=None,\n                description=\"Correctly extracted result from the model, if any, otherwise None\",\n            ),\n        ),\n        \"error\": (bool, Field(default=False)),\n        \"message\": (\n            Optional[str],\n            Field(\n                default=None,\n                description=\"Error message if no result was found, should be short and concise\",\n            ),\n        ),\n    }\n\n    MaybeModel = create_model(f\"Maybe{model.__name__}\", __base__=MaybeBase, **fields)\n\n    return MaybeModel  # type: ignore\n</code></pre>"},{"location":"api/#instructor.function_calls.Mode","title":"<code>Mode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>The mode to use for patching the client</p> Source code in <code>instructor/function_calls.py</code> <pre><code>class Mode(enum.Enum):\n    \"\"\"The mode to use for patching the client\"\"\"\n\n    FUNCTIONS: str = \"function_call\"\n    TOOLS: str = \"tool_call\"\n    JSON: str = \"json_mode\"\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema","title":"<code>OpenAISchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Augments a Pydantic model with OpenAI's schema for function calling</p> <p>This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call.</p>"},{"location":"api/#instructor.function_calls.OpenAISchema--usage","title":"Usage","text":"<pre><code>from instructor import OpenAISchema\n\nclass User(OpenAISchema):\n    name: str\n    age: int\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\n        \"content\": \"Jason is 20 years old\",\n        \"role\": \"user\"\n    }],\n    functions=[User.openai_schema],\n    function_call={\"name\": User.openai_schema[\"name\"]},\n)\n\nuser = User.from_response(completion)\n\nprint(user.model_dump())\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema--result","title":"Result","text":"<pre><code>{\n    \"name\": \"Jason Liu\",\n    \"age\": 20,\n}\n</code></pre> Source code in <code>instructor/function_calls.py</code> <pre><code>class OpenAISchema(BaseModel):\n    \"\"\"\n    Augments a Pydantic model with OpenAI's schema for function calling\n\n    This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call.\n\n    ## Usage\n\n    ```python\n    from instructor import OpenAISchema\n\n    class User(OpenAISchema):\n        name: str\n        age: int\n\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\n            \"content\": \"Jason is 20 years old\",\n            \"role\": \"user\"\n        }],\n        functions=[User.openai_schema],\n        function_call={\"name\": User.openai_schema[\"name\"]},\n    )\n\n    user = User.from_response(completion)\n\n    print(user.model_dump())\n    ```\n    ## Result\n\n    ```\n    {\n        \"name\": \"Jason Liu\",\n        \"age\": 20,\n    }\n    ```\n\n\n    \"\"\"\n\n    @classmethod\n    @property\n    def openai_schema(cls):\n        \"\"\"\n        Return the schema in the format of OpenAI's schema as jsonschema\n\n        Note:\n            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n        Returns:\n            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n        \"\"\"\n        schema = cls.model_json_schema()\n        docstring = parse(cls.__doc__ or \"\")\n        parameters = {\n            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n        }\n        for param in docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                if \"description\" not in parameters[\"properties\"][name]:\n                    parameters[\"properties\"][name][\"description\"] = description\n\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n\n        if \"description\" not in schema:\n            if docstring.short_description:\n                schema[\"description\"] = docstring.short_description\n            else:\n                schema[\"description\"] = (\n                    f\"Correctly extracted `{cls.__name__}` with all \"\n                    f\"the required parameters with correct types\"\n                )\n\n        return {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": parameters,\n        }\n\n    @classmethod\n    def from_response(\n        cls,\n        completion,\n        validation_context=None,\n        strict: bool = None,\n        mode: Mode = Mode.FUNCTIONS,\n        stream_multitask: bool = False,\n    ):\n        \"\"\"Execute the function from the response of an openai chat completion\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from an openai chat completion\n            throw_error (bool): Whether to throw an error if the function call is not detected\n            validation_context (dict): The validation context to use for validating the response\n            strict (bool): Whether to use strict json parsing\n            mode (Mode): The openai completion mode\n            stream_multitask (bool): Whether to stream a multitask response\n\n        Returns:\n            cls (OpenAISchema): An instance of the class\n        \"\"\"\n        if stream_multitask:\n            return cls.from_streaming_response(completion, mode)\n\n        message = completion.choices[0].message\n\n        if mode == Mode.FUNCTIONS:\n            assert (\n                message.function_call.name == cls.openai_schema[\"name\"]\n            ), \"Function name does not match\"\n            return cls.model_validate_json(\n                message.function_call.arguments,\n                context=validation_context,\n                strict=strict,\n            )\n        elif mode == Mode.TOOLS:\n            assert (\n                len(message.tool_calls) == 1\n            ), \"Instructor does not support multiple tool calls, use List[Model] instead.\"\n            tool_call = message.tool_calls[0]\n            assert (\n                tool_call.function.name == cls.openai_schema[\"name\"]\n            ), \"Tool name does not match\"\n            return cls.model_validate_json(\n                tool_call.function.arguments,\n                context=validation_context,\n                strict=strict,\n            )\n        elif mode == Mode.JSON:\n            return cls.model_validate_json(\n                message.content,\n                context=validation_context,\n                strict=strict,\n            )\n        else:\n            raise ValueError(f\"Invalid patch mode: {mode}\")\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema.openai_schema","title":"<code>openai_schema</code>  <code>classmethod</code> <code>property</code>","text":"<p>Return the schema in the format of OpenAI's schema as jsonschema</p> Note <p>Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.</p> <p>Returns:</p> Name Type Description <code>model_json_schema</code> <code>dict</code> <p>A dictionary in the format of OpenAI's schema as jsonschema</p>"},{"location":"api/#instructor.function_calls.OpenAISchema.from_response","title":"<code>from_response(completion, validation_context=None, strict=None, mode=Mode.FUNCTIONS, stream_multitask=False)</code>  <code>classmethod</code>","text":"<p>Execute the function from the response of an openai chat completion</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from an openai chat completion</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the function call is not detected</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>The openai completion mode</p> <code>FUNCTIONS</code> <code>stream_multitask</code> <code>bool</code> <p>Whether to stream a multitask response</p> <code>False</code> <p>Returns:</p> Name Type Description <code>cls</code> <code>OpenAISchema</code> <p>An instance of the class</p> Source code in <code>instructor/function_calls.py</code> <pre><code>@classmethod\ndef from_response(\n    cls,\n    completion,\n    validation_context=None,\n    strict: bool = None,\n    mode: Mode = Mode.FUNCTIONS,\n    stream_multitask: bool = False,\n):\n    \"\"\"Execute the function from the response of an openai chat completion\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from an openai chat completion\n        throw_error (bool): Whether to throw an error if the function call is not detected\n        validation_context (dict): The validation context to use for validating the response\n        strict (bool): Whether to use strict json parsing\n        mode (Mode): The openai completion mode\n        stream_multitask (bool): Whether to stream a multitask response\n\n    Returns:\n        cls (OpenAISchema): An instance of the class\n    \"\"\"\n    if stream_multitask:\n        return cls.from_streaming_response(completion, mode)\n\n    message = completion.choices[0].message\n\n    if mode == Mode.FUNCTIONS:\n        assert (\n            message.function_call.name == cls.openai_schema[\"name\"]\n        ), \"Function name does not match\"\n        return cls.model_validate_json(\n            message.function_call.arguments,\n            context=validation_context,\n            strict=strict,\n        )\n    elif mode == Mode.TOOLS:\n        assert (\n            len(message.tool_calls) == 1\n        ), \"Instructor does not support multiple tool calls, use List[Model] instead.\"\n        tool_call = message.tool_calls[0]\n        assert (\n            tool_call.function.name == cls.openai_schema[\"name\"]\n        ), \"Tool name does not match\"\n        return cls.model_validate_json(\n            tool_call.function.arguments,\n            context=validation_context,\n            strict=strict,\n        )\n    elif mode == Mode.JSON:\n        return cls.model_validate_json(\n            message.content,\n            context=validation_context,\n            strict=strict,\n        )\n    else:\n        raise ValueError(f\"Invalid patch mode: {mode}\")\n</code></pre>"},{"location":"api/#instructor.function_calls.openai_function","title":"<code>openai_function</code>","text":"<p>Decorator to convert a function into an OpenAI function.</p> <p>This decorator will convert a function into an OpenAI function. The function will be validated using pydantic and the schema will be generated from the function signature.</p> Example <pre><code>@openai_function\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n\ncompletion = openai.ChatCompletion.create(\n    ...\n    messages=[{\n        \"content\": \"What is 1 + 1?\",\n        \"role\": \"user\"\n    }]\n)\nsum.from_response(completion)\n# 2\n</code></pre> Source code in <code>instructor/function_calls.py</code> <pre><code>class openai_function:\n    \"\"\"\n    Decorator to convert a function into an OpenAI function.\n\n    This decorator will convert a function into an OpenAI function. The\n    function will be validated using pydantic and the schema will be\n    generated from the function signature.\n\n    Example:\n        ```python\n        @openai_function\n        def sum(a: int, b: int) -&gt; int:\n            return a + b\n\n        completion = openai.ChatCompletion.create(\n            ...\n            messages=[{\n                \"content\": \"What is 1 + 1?\",\n                \"role\": \"user\"\n            }]\n        )\n        sum.from_response(completion)\n        # 2\n        ```\n    \"\"\"\n\n    def __init__(self, func: Callable) -&gt; None:\n        self.func = func\n        self.validate_func = validate_arguments(func)\n        self.docstring = parse(self.func.__doc__ or \"\")\n\n        parameters = self.validate_func.model.model_json_schema()\n        parameters[\"properties\"] = {\n            k: v\n            for k, v in parameters[\"properties\"].items()\n            if k not in (\"v__duplicate_kwargs\", \"args\", \"kwargs\")\n        }\n        for param in self.docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                parameters[\"properties\"][name][\"description\"] = description\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n        self.openai_schema = {\n            \"name\": self.func.__name__,\n            \"description\": self.docstring.short_description,\n            \"parameters\": parameters,\n        }\n        self.model = self.validate_func.model\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        @wraps(self.func)\n        def wrapper(*args, **kwargs):\n            return self.validate_func(*args, **kwargs)\n\n        return wrapper(*args, **kwargs)\n\n    def from_response(self, completion, throw_error=True, strict: bool = None):\n        \"\"\"\n        Parse the response from OpenAI's API and return the function call\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from OpenAI's API\n            throw_error (bool): Whether to throw an error if the response does not contain a function call\n\n        Returns:\n            result (any): result of the function call\n        \"\"\"\n        message = completion[\"choices\"][0][\"message\"]\n\n        if throw_error:\n            assert \"function_call\" in message, \"No function call detected\"\n            assert (\n                message[\"function_call\"][\"name\"] == self.openai_schema[\"name\"]\n            ), \"Function name does not match\"\n\n        function_call = message[\"function_call\"]\n        arguments = json.loads(function_call[\"arguments\"], strict=strict)\n        return self.validate_func(**arguments)\n</code></pre>"},{"location":"api/#instructor.function_calls.openai_function.from_response","title":"<code>from_response(completion, throw_error=True, strict=None)</code>","text":"<p>Parse the response from OpenAI's API and return the function call</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from OpenAI's API</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the response does not contain a function call</p> <code>True</code> <p>Returns:</p> Name Type Description <code>result</code> <code>any</code> <p>result of the function call</p> Source code in <code>instructor/function_calls.py</code> <pre><code>def from_response(self, completion, throw_error=True, strict: bool = None):\n    \"\"\"\n    Parse the response from OpenAI's API and return the function call\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from OpenAI's API\n        throw_error (bool): Whether to throw an error if the response does not contain a function call\n\n    Returns:\n        result (any): result of the function call\n    \"\"\"\n    message = completion[\"choices\"][0][\"message\"]\n\n    if throw_error:\n        assert \"function_call\" in message, \"No function call detected\"\n        assert (\n            message[\"function_call\"][\"name\"] == self.openai_schema[\"name\"]\n        ), \"Function name does not match\"\n\n    function_call = message[\"function_call\"]\n    arguments = json.loads(function_call[\"arguments\"], strict=strict)\n    return self.validate_func(**arguments)\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We would love for you to contribute to <code>Instructor</code>.</p>"},{"location":"contributing/#evals","title":"Evals","text":"<p>We invite you to contribute evals in pytest as a way to monitor the quality of the openai models and the instructor library. To get started check out the jxnl/instructor/tests/evals and contribute your own evals in the form of pytest tests. These evals will be run once a week and the results will be posted.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>If you find a bug, please file an issue on our issue tracker on GitHub.</p> <p>To help us reproduce the bug, please provide a minimal reproducible example, including a code snippet and the full error message.</p> <ol> <li>The <code>response_model</code> you are using.</li> <li>The <code>messages</code> you are using.</li> <li>The <code>model</code> you are using.</li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>We welcome pull requests! There is plenty to do, and we are happy to discuss any contributions you would like to make.</p> <p>If it is not a small change, please start by filing an issue first.</p> <p>If you need ideas, you can check out the help wanted or good first issue labels.</p> <p>Grit is used to enforce best practices. You can run <code>grit check</code> to check your code before submitting a pull request.</p>"},{"location":"contributing/#contributors","title":"Contributors","text":""},{"location":"help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p>"},{"location":"help/#concepts","title":"Concepts","text":"<p>The concepts section explains the core concepts of Instructor and how to prompt with models.</p>"},{"location":"help/#cookbooks","title":"Cookbooks","text":"<p>The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.</p>"},{"location":"help/#blog","title":"Blog","text":"<p>The blog contains articles that explain how to use Instructor in different scenarios.</p>"},{"location":"help/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub discussions are useful for asking questions, your question and the answer will help everyone.</p>"},{"location":"help/#github-issues","title":"GitHub Issues","text":"<p>GitHub issues are useful for reporting bugs or requesting new features.</p>"},{"location":"help/#twitter","title":"Twitter","text":"<p>You can also reach out to me on Twitter if you have any questions or ideas.</p>"},{"location":"installation/","title":"Installation","text":"<p>Installation is as simple as:</p> <pre><code>pip install instructor\n</code></pre> <p>Instructor has a few dependencies:</p> <ul> <li><code>openai</code>: OpenAI's Python client.</li> <li><code>typer</code>: Build great CLIs. Easy to code. Based on Python type hints.</li> <li><code>docstring-parser</code>: A parser for Python docstrings, to improve the experience of working with docstrings in jsonschema.</li> <li><code>pydantic</code>: Data validation and settings management using python type annotations.</li> </ul> <p>If you've got Python 3.9+ and <code>pip</code> installed, you're good to go.</p>"},{"location":"why/","title":"Why use Instructor?","text":"Why use Pydantic? <p>Its hard to answer the question of why use Instructor without first answering why use Pydantic.:</p> <ul> <li> <p>Powered by type hints \u2014 with Pydantic, schema validation and serialization are controlled by type annotations; less to learn, less code to write, and integration with your IDE and static analysis tools.</p> </li> <li> <p>Speed \u2014 Pydantic's core validation logic is written in Rust. As a result, Pydantic is among the fastest data validation libraries for Python.</p> </li> <li> <p>JSON Schema \u2014 Pydantic models can emit JSON Schema, allowing for easy integration with other tools. [Learn more\u2026]</p> </li> <li> <p>Customisation \u2014 Pydantic allows custom validators and serializers to alter how data is processed in many powerful ways.</p> </li> <li> <p>Ecosystem \u2014 around 8,000 packages on PyPI use Pydantic, including massively popular libraries like FastAPI, huggingface, Django Ninja, SQLModel, &amp; LangChain.</p> </li> <li> <p>Battle tested \u2014 Pydantic is downloaded over 70M times/month and is used by all FAANG companies and 20 of the 25 largest companies on NASDAQ. If you're trying to do something with Pydantic, someone else has probably already done it.</p> </li> </ul> <p>Our <code>instructor.patch</code> for the <code>OpenAI</code> class introduces three key enhancements:</p> <ul> <li>Response Mode: Specify a Pydantic model to streamline data extraction.</li> <li>Max Retries: Set your desired number of retry attempts for requests.</li> <li>Validation Context: Provide a context object for enhanced validator access.   A Glimpse into Instructor's Capabilities</li> </ul> <p>Using Validators</p> <p>Learn more about validators checkout our blog post Good llm validation is just good validation</p> <p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek:</p>"},{"location":"why/#understanding-the-patch","title":"Understanding the <code>patch</code>","text":"<p>Lets go over the <code>patch</code> function. And see how we can leverage it to make use of instructor</p>"},{"location":"why/#step-1-patch-the-client","title":"Step 1: Patch the client","text":"<p>First, import the required libraries and apply the patch function to the OpenAI module. This exposes new functionality with the response_model parameter.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())\n</code></pre>"},{"location":"why/#step-2-define-the-pydantic-model","title":"Step 2: Define the Pydantic Model","text":"<p>Create a Pydantic model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>from pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n</code></pre>"},{"location":"why/#step-3-extract","title":"Step 3: Extract","text":"<p>Use the <code>client.chat.completions.create</code> method to send a prompt and extract the data into the Pydantic object. The response_model parameter specifies the Pydantic model to use for extraction. Its helpful to annotate the variable with the type of the response model. which will help your IDE provide autocomplete and spell check.</p> <pre><code>user: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n)\n\nassert user.name == \"Jason\"\nassert user.age == 25\n</code></pre>"},{"location":"why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>from pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>Its important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(OpenAI())\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n</code></pre> <p>As you can see, we've baked in a self correcting mechanism into the model. This is a powerful way to make your models more robust and less brittle without include a lot of extra code or prompt.</p>"},{"location":"blog/","title":"Welcome to the Instructor Blog","text":"<p>The goal of the blog is to capture some content that does not neatly fit within documentation or the cookbooks.</p>"},{"location":"blog/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>What is Query Understanding, how does it go beyond embeddings?</li> <li>How can one achieve GPT-4 level summaries using GPT-3.5-turbo?</li> <li>What are the basics of Guardrails and Validation in AI models?</li> <li>How does one validate citations in AI-generated content?</li> <li>What are the methods and benefits of fine-tuning and distillation in AI models?</li> </ol>"},{"location":"blog/#learning-python","title":"Learning Python","text":"<ul> <li>How can I effectively cache my functions in Python?</li> <li>What are the fundamentals of batch processing with async in Python?</li> <li>How can I stream models to improve latency?</li> </ul>"},{"location":"blog/#talks","title":"Talks","text":"<ul> <li>What were the key insights and topics covered at the AI Engineering Summit 2023?</li> </ul>"},{"location":"blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/","title":"AI Engineer Keynote: Pydantic is all you need","text":"<p>Click here to watch the full talk</p> <p>Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts, </p> <p>I'd genuinely appreciate any feedback on the talk \u2013 every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible.</p>","tags":["python","talks","prompt engineering","video"]},{"location":"blog/2023/11/26/python-caching/","title":"Introduction to Caching in Python","text":"<p>Instructor makes working with language models easy, but they are still computationally expensive.</p> <p>Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with <code>pickle</code>, and explore solutions that use <code>decorators</code> like <code>functools.cache</code>. Then, we'll craft custom decorators with <code>diskcache</code> and <code>redis</code> to support persistent caching and distributed systems.</p> <p>Lets first consider our canonical example, using the <code>OpenAI</code> Python client to extract user details.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Enables `response_model`\nclient = instructor.patch(OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": data},\n    ]\n)\n</code></pre> <p>Now imagine batch processing data, running tests or experiments, or simply calling <code>extract</code> multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import functools\n\n@functools.cache\ndef extract(data):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <pre><code>import time\n\nstart = time.perf_counter() # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\") # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\n&gt;&gt;&gt; Time taken: 0.9267581660533324\n&gt;&gt;&gt; Time taken: 1.2080417945981026e-06 # (3)\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> <li>The second call to <code>extract</code> is much faster because the result is returned from the cache!</li> </ol> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\") # (1)\n        result = func(*args, **kwargs)\n        print(\"Do something after\") # (2)\n        return result\n    return wrapper\n\n@decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n&gt;&gt;&gt; \"Do something before\"\n&gt;&gt;&gt; \"Hello!\"\n&gt;&gt;&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory') # (1)\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel): # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation # (4)\n    if not issubclass(return_type, BaseModel): # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\" #  (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport json\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel): # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\" # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementatino is the same, but we're using a different caching backend!</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#conclusion","title":"Conclusion","text":"<p>Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead.</p> <p>If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/05/chain-of-density/","title":"Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density","text":"<p>Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor</p> <p>In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density.</p> <p>By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density [Adams et al. (2023)]. As always, all code is readily available in our <code>examples/chain-of-density</code> folder in our repo for your reference.</p> Datasets and Colab Notebook <p>We've also uploaded all our generated data to Hugging Face here for you to use if you'd like to try reproducing these experiments. We've also added a Colab Instance for you to check our generated values.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-1-chain-of-density","title":"Part 1) Chain of Density","text":"<p>Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries.</p> <p>Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.</p> <p>First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators.</p> Implementation Details <p>Note that our implementation uses a validator to ensure that the rewritten summary has a minimum length rather than a prompt. We also perform just 3 and not 5 rounds of rewrites, resulting in a lower final entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#original-prompt","title":"Original Prompt","text":"<p>We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want.</p> Original Chain of Density Prompt <pre><code>Article: {{ARTICLE}}\n\nYou will generate increasingly concise, entity-dense summaries of the\nabove Article.\n\nRepeat the following 2 steps 5 times.\n\nStep 1. Identify 1-3 informative Entities (\";\" delimited) from the\nArticle which are missing from the previously generated summary.\nStep 2. Write a new, denser summary of identical length which covers\nevery entity and detail from the previous summary plus the Missing\nEntities.\n\nA Missing Entity is:\n- Relevant: to the main story.\n- Specific: descriptive yet concise (5 words or fewer).\n- Novel; not in the previous summary.\n- Faithful: present in the Article.\n- Anywhere: located anywhere in the Article.\n\nGuidelines:\n- The first summary should be long (4-5 sentences, -80 words) yet\nhighly non-specific, containing little information beyond the\nentities marked as missing. Use overly verbose language and fillers\n(e.g., \"this article discusses\") to reach -80 words.\n- Make every word count: re-write the previous summary to improve\nflow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative\nphrases like \"the article discusses\"\n- The summaries should become highly dense and concise yet\nself-contained, e.g., easily understood without the Article.\n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be\nmade, add fewer new entities.\n\nRemember, use the exact same number of words for each summary.\n\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries\nwhose keys are \"Missing_Entities\" and \"Denser_Summary\"\n</code></pre> <p> </p> Improved process with Instructor","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#data-modelling","title":"Data Modelling","text":"<p>Before we begin modelling the data, let's make sure we install all of our dependencies</p> <pre><code>pip install instructor aiohttp rich\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#initial-summary","title":"Initial Summary","text":"<p>Let's start by walking through some of the data models that we'll be using as the <code>response_model</code> for our open ai function calls</p> <p>Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs.</p> A quick note on Docstrings <p>Under the hood, Instructor parses the <code>response_model</code> that you give us into a function call for OpenAI to execute. This means that the final output will be closely linked to the Pydantic model you specify.</p> <p>For instance, this simple model that we later use in fine-tuning.</p> <pre><code>class GeneratedSummary(BaseModel):\n\"\"\"\nThis represents a highly concise summary that includes as many entities as possible from the original source article.\n\nAn Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\nGuidelines\n- Make every word count\n- The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n\"\"\"\n\nsummary: str = Field(\n    ...,\n    description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n)\n</code></pre> <p>We eventually transform it into an OpenAI function call as seen below.</p> <pre><code>{\n\"functions\": [\n    {\n    \"name\": \"GeneratedSummary\",\n    \"description\": \"This represents a highly concise summary that includes as many entities as possible from the original source article.\\n\\nAn Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\\n\\nGuidelines\\n- Make every word count\\n- The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\\n- Make space with fusion, compression, and removal of uninformative phrases like \\\"the article discusses\\\"\",\n    \"parameters\": {\n        \"properties\": {\n        \"summary\": {\n            \"description\": \"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n            \"title\": \"Summary\",\n            \"type\": \"string\"\n        }\n        },\n        \"required\": [\n        \"summary\"\n        ],\n        \"type\": \"object\"\n    }\n    }\n]\n}\n}\n</code></pre> <p>Therefore this means that the more elaborate and detailed your descriptions are, the better the outputs you will be able to get back. But we don't just stop there, since it's all Pydantic under the hood, you can validate and parse the resulting output to make sure it is exactly what you specify. It's all python all the way down.</p> <pre><code>class InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\",\n    )\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#rewritten-summary","title":"Rewritten Summary","text":"<p>We'll also need one additional class to help model the rewritten schema</p> <pre><code>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n</code></pre> <p>Using Pydantic Validators with Instructor</p> <p>For a more in-depth walkthrough on how to use <code>Pydantic</code> validators with the <code>Instructor</code> library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation</p> <p>Ideally, we'd like for <code>Missing</code> to have a length between 1 and 3, <code>Absent</code> to be an empty list and for our rewritten summaries to keep a minimum entity density. With <code>Instructor</code>, we can implement this logic using native <code>Pydantic</code> validators that are simply declared as part of the class itself.</p> <pre><code>import nltk\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n@field_validator(\"summary\")\ndef min_length(cls, v: str):\n    tokens = nltk.word_tokenize(v) #(1)!\n    num_tokens = len(tokens)\n    if num_tokens &lt; 60:\n        raise ValueError(\n            \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n        )\n    return v\n\n@field_validator(\"missing\")\ndef has_missing_entities(cls, missing_entities: List[str]):\n    if len(missing_entities) == 0:\n        raise ValueError(\n            \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n        )\n    return missing_entities\n\n@field_validator(\"absent\")\ndef has_no_absent_entities(cls, absent_entities: List[str]):\n    absent_entity_string = \",\".join(absent_entities)\n    if len(absent_entities) &gt; 0:\n        print(f\"Detected absent entities of {absent_entity_string}\")\n        raise ValueError(\n            f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n        )\n    return absent_entities\n\n@field_validator(\"summary\")\n    def min_entity_density(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n\n        # Extract Entities\n        doc = nlp(v) #(2)!\n        num_entities = len(doc.ents)\n\n        density = num_entities / num_tokens\n        if density &lt; 0.08: #(3)!\n            raise ValueError(\n                f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n            )\n\n        return v\n</code></pre> <ol> <li> <p>Similar to the original paper, we utilize the <code>NLTK</code> word tokenizer to count the number of tokens within our generated sentences.     We aim for at least 60 tokens in our generated summary so that we don't lose information.</p> </li> <li> <p>We also use the spaCy library to calculate the entity density of the generated summary.</p> </li> <li> <p>We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case</p> </li> </ol>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#putting-it-all-together","title":"Putting it all Together","text":"<p>Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using <code>Chain Of Density</code> summarization.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI()) #(1)!\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(  # (2)!\n        model=\"gpt-4-0613\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create( # (3)!\n            model=\"gpt-4-0613\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3, #(4)!\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n</code></pre> <ol> <li> <p>We need to apply a <code>patch</code> function on the <code>OpenAI</code> client for us to get all     of the benefits that <code>Instructor</code> provides. With a simple <code>patch</code>, we can get     automatic type coercion of our outputs and automatic retries for invalid outputs     out of the box!</p> </li> <li> <p>We first generate an initial summary. Note here that we explictly ask for a summary that has     80 words and is lengthy with overly verbose fillers in the system prompt</p> </li> <li> <p>We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary.     Using <code>Instructor</code>, we also get validation of the generated output with our <code>field_validator</code>s that we defined above</p> </li> <li> <p>If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites</p> </li> </ol> <p>This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural.</p> <p>First Iteration</p> <p>This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event.</p> <p>Final Iteration</p> <p>Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-2-fine-tuning","title":"Part 2) Fine-Tuning","text":"<p>In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of <code>GPT-4</code> to see how it stacks up.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-a-training-set","title":"Creating a Training Set","text":"<p>In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the <code>griffin/chain-of-density</code> dataset and split these articles into a <code>train.csv</code> and a <code>test.csv</code> file which we uploaded to Hugging Face. Now, we just neeed to import the <code>Instructions</code> module from the <code>Instructor</code> package which allows you to generate a nicely formatted <code>.jsonl</code> file to be used for fine-tuning</p> <pre><code>from typing import List\nfrom chain_of_density import summarize_article #(1)!\nimport csv\nimport logging\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI()) # (2)!\n\nlogging.basicConfig(level=logging.INFO) #(3)!\n\ninstructions = instructor.Instructions( #(4)!\n    name=\"Chain Of Density\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"generated.jsonl\")],\n    openai_client=client,\n)\n\nclass GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n\n@instructions.distil #(4)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1]) #(5)!\n\nwith open(\"train.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    next(reader)  # Skip the header\n    for article, summary in reader:\n        # Run Distillisation to generate the values\n        distil_summarization(article)\n</code></pre> <ol> <li> <p>In this example, we're using the summarize_article that we defined up above. We saved it in a local file called <code>chain_of_density.py</code>,     hence the import</p> </li> <li> <p>We patch the default OpenAI client so that we can use the Instructor library with it</p> </li> <li> <p>We also need to configure logging at the <code>INFO</code> level. This is very important, if this is not configured, your output will not be generated.</p> </li> <li> <p>We instantiate a <code>Instruction</code> object which will help us handle the conversion of our function calls into a valid <code>.jsonl</code> file. We also define     the name of the <code>.jsonl</code> file in the <code>log_handlers</code> parameter</p> </li> <li> <p>We add in an <code>instructions.distil</code> annotation so that we automatically capture the input and output of the function we'd like to     fine-tune our model to output</p> </li> <li> <p>We return a <code>Pydantic</code> object which matches the annotation that we use on our function. Note that we must specify a <code>Pydantic</code> object to     be returned when using the <code>instructions.distil</code> annotation</p> </li> </ol> <p>Rate Limiting</p> <p>We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with <code>tenacity</code> and set the <code>OPENAI_API_KEY</code> shell environment variable before running any subsequent commands</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-fine-tuning-jobs","title":"Creating Fine-Tuning Jobs","text":"<p>Once we run this script, we'll have a new file called <code>generated.jsonl</code> in our local repository. Now all that's left is to run the command below to start fine-tuning your first model!</p> <pre><code>instructor jobs create-from-file generated.jsonl\n</code></pre> Finetuning Reference <p>Checking out our Finetuning CLI to learn about other hyperparameters that you can tune to improve your model's performance.</p> <p>Once the job is complete, all we need to do is to then change the annotation in the function call to <code>distil_summarization</code> in our original file above to start using our new model.</p> <pre><code>@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") #(1)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1])\n</code></pre> <ol> <li>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of    ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard <p>With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#results-and-benchmarks","title":"Results and Benchmarks","text":"<p>We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning.</p> <ul> <li>Entity Density : This is entities per token, the higher the better for density.</li> <li>Latency : Time to last token generated in seconds</li> <li>Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference</li> </ul> <code>3.5 Finetuned (n)</code> <p>This is a GPT 3.5 model that we fine-tuned on <code>n</code> examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler )</p> <code>GPT-4 (COD)</code> <p>This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above</p> <code>GPT-3.5 (Vanilla)</code> <p>This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens.</p> Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets <p>For our finetuned models, we did a few optimisations to raise the performance.</p> <p>We only included summaries that had a minimum density of 0.15 in the dataset, took the summary in the entire chain with the highest density as the final one, forced every regenerated summary to have a minimum density of 0.12 and regenerated summaries up to three times if they didn't meet the summaries. This is a much more expensive strategy and can cost up to 2.5x or more what we do in this tutorial</p> <p>This resulted in the total cost of $63.46 to generate just 75 examples due to the stringent requirements, translating to about $0.85 per generated summary example.</p> <p>Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below.</p> Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 <p>Here, we can see that <code>GPT-4</code> has an approximate inference cost of <code>0.65</code> per summary while our finetuned models have an inference cost of <code>0.0091</code> per summary which is ~ <code>72x</code> cheaper.</p> <p>Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#conclusions","title":"Conclusions","text":"<p>Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models.</p> <p>We've seen how <code>Instructor</code> can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out <code>instructor</code> check out the github and don't forget to give us a star!</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/18/validate-citations/","title":"Verifying LLM Citations with Pydantic","text":"<p>Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification.</p> <p>We'll start with using a simple substring check to verify citations. Then we'll use <code>instructor</code> itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-1-simple-substring-check","title":"Example 1: Simple Substring Check","text":"<p>In this example, we use the <code>Statements</code> class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example","title":"Code Example:","text":"<pre><code>from typing import List, Optional\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field, ValidationError, ValidationInfo, field_validator, model_validator\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @field_validator(\"substring_quote\")\n    @classmethod\n    def substring_quote_exists(cls, v: str, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        for text_chunk in context.values():\n            if v in text_chunk: # (1)\n                return v\n        raise ValueError(\"Could not find substring_quote `{v}` in contexts\")\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <ol> <li>While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance.</li> </ol> <p>Once the class is defined, we can use it to validate the context and raise an error if the substring is not found.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example","title":"Error Message Example:","text":"<pre><code>answer.0.substring_quote\n  Value error, Could not find substring_quote `Paris is the capital of France` in contexts [type=value_error, input_value='Paris is the capital of France', input_type=str]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre> <p>Pydantic raises a validation error when the <code>substring_quote</code> attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-2-using-llm-for-verification","title":"Example 2: Using LLM for Verification","text":"<p>This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_1","title":"Code Example:","text":"<pre><code>class Validation(BaseModel):\n    is_valid: bool\n    error_messages: Optional[str] = Field(None, description=\"Error messages if any\")\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @model_validator(mode=\"after\")\n    def substring_quote_exists(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following citation exist in the following context?\\n\\nCitation: {self.substring_quote}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <p>Now when we use a correct citation, the LLM returns a valid response.</p> <pre><code>resp = AnswerWithCitaton.model_validate(\n    {\n        \"question\": \"What is the capital of France?\",\n        \"answer\": [\n            {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n        ],\n    },\n    context={\n        \"text_chunks\": {\n            1: \"Jason is a pirate\",\n            2: \"Paris is the capital of France\",\n            3: \"Irrelevant data\",\n        }\n    },\n)\nprint(resp.model_dump_json(indent=2))\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#result","title":"Result:","text":"<pre><code>{\n  \"question\": \"What is the capital of France?\",\n  \"answer\": [\n    {\n      \"body\": \"Paris\",\n      \"substring_quote\": \"Paris is the capital of France\"\n    }\n  ]\n}\n</code></pre> <p>When we have citations that don't exist in the context, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_1","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\nanswer.0\n  Value error, Citation not found in context [type=value_error, input_value={'body': 'Paris', 'substr... the capital of France'}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-3-aligning-citations-and-answers","title":"Example 3: Aligning Citations and Answers","text":"<p>In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment.</p> <p>We use the same <code>Statements</code> model as above, but we add a new model for the answer that also verifies the alignment of citations.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_2","title":"Code Example:","text":"<pre><code>class AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n\n    @model_validator(mode=\"after\")\n    def validate_answer(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following answers match the question and the context?\\n\\nQuestion: {self.question}\\n\\nAnswer: {self.answer}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n</code></pre> <p>When we have a mismatch between the answer and the citation, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Texas\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_2","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\n  Value error, The answer does not match the question and context [type=value_error, input_value={'question': 'What is the...he capital of France'}]}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#conclusion","title":"Conclusion","text":"<p>These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/","title":"Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation","text":"","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#introduction","title":"Introduction","text":"<p>Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the <code>instructor.instructions</code> streamlines this process, making the task you want to distil  more efficient and powerful while preserving its original functionality and backwards compatibility.</p> <p>If you want to see the full example checkout examples/distillation</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-use-instructor","title":"Why use Instructor?","text":"<p>Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where <code>Instructor</code> comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs! \n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")]\n)\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n# Generate some data\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\"role\": \"assistant\", \n            \"function_call\": \n                {\n                    \"name\": \"Multiply\", \n                    \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}'\n            }\n        }\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ]\n}\n</code></pre> <p>Run a finetune like this:</p> <p>Don't forget to set your OpenAI Key as an environment variable</p> <p>All of the <code>instructor jobs</code> commands assume you've set an environment variable of <code>OPENAI_API_KEY</code> in your shell. You can set this by running the command <code>export OPENAI_API_KEY=&lt;Insert API Key Here&gt;</code> in your shell</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#next-steps-and-future-plans","title":"Next Steps and Future Plans","text":"<p>Here's a sneak peek of what I'm planning:</p> <pre><code>from instructor import Instructions, patch\n\npatch() #(1)!\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") # (2)!\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <ol> <li> <p>Don't forget to run the <code>patch()</code> command that we provide with the <code>Instructor</code> package. This helps     automatically serialize the content back into the `Pydantic`` model that we're looking for.</p> </li> <li> <p>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id     of <code>ft:gpt-3.5-turbo-0613:personal::&lt;id&gt;</code> under their Fine-tuning tab on their dashboard</p> </li> </ol> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#conclusion","title":"Conclusion","text":"<p>We've seen how <code>Instructor</code> can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/","title":"Generators and LLM Streaming","text":"<p>Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.</p> <p>And what makes streaming possible? Generators!</p> <p>In this post, we're going to dive into the cool world of Python generators \u2014 these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#python-generators-an-efficient-approach-to-iterables","title":"Python Generators: An Efficient Approach to Iterables","text":"<p>Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#the-basics-yielding-values","title":"The Basics: Yielding Values","text":"<p>A generator function in Python uses the <code>yield</code> keyword. It yields values one at a time, allowing the function to pause and resume its state.</p> <pre><code>def count_to_3():\n    yield 1\n    yield 2\n    yield 3\n\nfor num in count_to_3():\n    print(num)\n</code></pre> <pre><code>1\n2\n3\n</code></pre>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#advantages-over-traditional-collections","title":"Advantages Over Traditional Collections","text":"<ul> <li>Lazy Evaluation &amp; reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first.</li> <li>Memory Efficiency: Only one item is in memory at a time.</li> <li>Maintain State: Automatically maintains state between executions.</li> </ul> <p>Let's see how much faster generators are and where they really shine:</p> <pre><code>import time\n\ndef expensive_func(x):\n    \"\"\"Simulate an expensive operation.\"\"\"\n    time.sleep(1)\n    return x ** 2\n\ndef calculate_time_for_first_result_with_list(func_input, func):\n    \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = [func(x) for x in func_input][0]\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n    return result\n\ndef calculate_time_for_first_result_with_generator(func_input, func):\n    \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = next(func(x) for x in func_input)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    return result\n\n# Prepare inputs for the function\nnumbers = [1, 2, 3, 4, 5]\n\n# Benchmarking\nfirst_result_list = calculate_time_for_first_result_with_list(numbers, expensive_func)\nfirst_result_gen = calculate_time_for_first_result_with_generator(numbers, expensive_func)\n</code></pre> <pre><code>Time for first result (list): 5.02 seconds\nTime for first result (generator): 1.01 seconds\n</code></pre> <p>The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#generator-expressions-a-shortcut","title":"Generator Expressions: A Shortcut","text":"<p>Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses.</p> <pre><code>squares = (x*x for x in range(10))\n</code></pre>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#use-cases-in-real-world-applications","title":"Use Cases in Real-World Applications","text":"<p>Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#llm-streaming","title":"LLM Streaming","text":"<p>If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators.</p> <p>Here's how a vanilla openai generator looks:</p> <pre><code>from openai import OpenAI\n\n# Set your OpenAI API key\nclient = OpenAI(\n    api_key=\"My API Key\",\n)\n\nresponse_generator = client.chat.completions.create(\n    model='gpt-3.5-turbo',\n    messages=[\n        {'role': 'user', 'content': \"What are some good reasons to smile?\"}\n    ],\n    temperature=0,\n    stream=True\n)\n\nfor chunk in response_generator:\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre> <p>This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM.</p> <p>Should we wait for the entire stream to finish before extracting &amp; validating the list of components or can we extract &amp; validate the components in real time as they are streamed?</p> <p>In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ).</p> <p>Let's see how we can use Instructor to handle extraction from this real time stream!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#e-commerce-product-ranking","title":"E-commerce Product Ranking","text":"","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#scenario","title":"Scenario","text":"<p>Imagine an e-commerce platform where we have:</p> <p>\u2022 a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions.</p> <p>\u2022 a list of candidate products: these could be some shortlisted products we think the customer would like.</p> <p>Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#stream-processing","title":"Stream Processing","text":"<p>User Data:</p> <p>Let's assume we have the following user profile:</p> <pre><code>profile_data = \"\"\"\nCustomer ID: 12345\nRecent Purchases: [Laptop, Wireless Headphones, Smart Watch]\nFrequently Browsed Categories: [Electronics, Books, Fitness Equipment]\nProduct Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars}\nRecent Search History: [best budget laptops 2023, latest sci-fi books, yoga mats]\nPreferred Brands: [Apple, AllBirds, Bench]\nResponses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested}\nLoyalty Program Status: Gold Member\nAverage Monthly Spend: $500\nPreferred Shopping Times: Weekend Evenings\n...\n\"\"\"\n</code></pre> <p>We want to rank the following products for this user:</p> <pre><code>products = [\n    {\"product_id\": 1, \"product_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\"},\n    {\"product_id\": 2, \"product_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\"},\n    {\"product_id\": 3, \"product_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\"},\n    {\"product_id\": 4, \"product_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\"},\n    {\"product_id\": 5, \"product_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\"},\n    {\"product_id\": 6, \"product_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\"},\n    {\"product_id\": 7, \"product_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\"},\n    {\"product_id\": 8, \"product_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\"},\n    {\"product_id\": 9, \"product_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\"},\n    {\"product_id\": 10, \"product_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\"}\n]\n</code></pre> <p>Let's now define our models for structured extraction. Note: instructor will conveniently let us use <code>Iterable</code> to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on <code>Iterable</code> to define what we ultimately want - a (ranked) list of product recommendations.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\nclass ProductRecommendation(BaseModel):\n    product_id: str\n    product_name: str\n\nRecommendations = Iterable[ProductRecommendation]\n</code></pre> <p>Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to <code>True</code> and process each product recommendation as it comes in:</p> <pre><code>prompt = f\"Based on the following user profile:\\n{profile_data}\\nRank the following products from most relevant to least relevant:\\n\" + '\\n'.join(f\"{product['product_id']} {product['product_name']}\" for product in products)\n\nstart_perf = time.perf_counter()\nrecommendations_stream = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=True,\n    messages=[\n        {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n)\nfor product in recommendations_stream:\n    print(product)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    break\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (generator): 4.33 seconds\n</code></pre> <p><code>recommendations_stream</code> is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare.</p> <pre><code>start_perf = time.perf_counter()\nrecommendations_list = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=False,\n    messages=[\n        {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n)\nprint(recommendations_list[0])\nend_perf = time.perf_counter()\nprint(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (list): 8.63 seconds\n</code></pre> <p>Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#fastapi","title":"FastAPI","text":"<p>We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here! </p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#key-takeaways","title":"Key Takeaways","text":"<p>To summarize, we looked at:</p> <p>\u2022   Generators in Python: A powerful feature that allows for efficient data handling with reduced latency</p> <p>\u2022   LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw!</p> <p>Don't forget to check our GitHub for more resources and give us a star if you find the library helpful!</p> <p>If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/","title":"Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls","text":"<p>Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-problem-with-existing-llm-frameworks","title":"The Problem with Existing LLM Frameworks","text":"<p>Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-openai-function-calling-game-changer","title":"The OpenAI Function Calling Game-Changer","text":"<p>OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#why-pydantic","title":"Why Pydantic?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and the language model.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul> <pre><code>import pydantic\nimport instructor\nfrom openai import OpenAI\n\n# Enables the response_model\nclient = instructor.patch(OpenAI())\n\nclass UserDetail(pydantic.BaseModel):\n    name: str\n    age: int\n\n    def introduce(self):\n        return f\"Hello I'm {self.name} and I'm {self.age} years old\"\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n)\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#simplifying-validation-flow-with-pydantic","title":"Simplifying Validation Flow with Pydantic","text":"<p>Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks.</p> <pre><code>from typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\nfrom instructor import llm_validator, patch\n\nfrom openai import OpenAI\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\")\n        ),\n    ]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-modular-approach","title":"The Modular Approach","text":"<p>Pydantic allows for modular output schemas. This leads to more organized code.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#composition-of-schemas","title":"Composition of Schemas","text":"<pre><code>class UserDetails(BaseModel):\n    name: str\n    age: int\n\nclass UserWithAddress(UserDetails):\n    address: str\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#defining-relationships","title":"Defining Relationships","text":"<pre><code>class UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    friends: List[int]\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-enums","title":"Using Enums","text":"<pre><code>from enum import Enum, auto\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#flexible-schemas","title":"Flexible Schemas","text":"<pre><code>from typing import List\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#chain-of-thought","title":"Chain of Thought","text":"<pre><code>class TimeRange(BaseModel):\n    chain_of_thought: str\n    start_time: int\n    end_time: int\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    work_time: TimeRange\n    leisure_time: TimeRange\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#language-models-as-microservices","title":"Language Models as Microservices","text":"<p>The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#fastapi-stub","title":"FastAPI Stub","text":"<pre><code>app = FastAPI()\n\n@app.get(\"/user/{user_id}\", response_model=UserDetails)\nasync def get_user(user_id: int) -&gt; UserDetails:\n    return UserDetails(...)\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-instructor-as-a-function","title":"Using Instructor as a Function","text":"<pre><code>def extract_user(str) -&gt; UserDetails:\n    return client.chat.completions(\n           response_model=UserDetails,\n           messages=[...]\n    )\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#response-modeling","title":"Response Modeling","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[UserDetail]\n    error: bool\n    message: Optional[str]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#conclusion","title":"Conclusion","text":"<p>Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["Introduction"]},{"location":"blog/2023/11/13/learn-async/","title":"Introduction to Batch Processing using <code>asyncio</code> and <code>Instructor</code>","text":"<p>Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using <code>instructor</code> and learn how to use <code>asyncio.gather</code> and <code>asyncio.as_completed</code> for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using <code>asyncio.Semaphore</code>.</p> <p>Github Example</p> <p>If you want to run the code examples in this article, you can find them on jxnl/instructor</p> <p>We will start by defining an <code>async</code> function that calls <code>openai</code> to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-asyncio","title":"Understanding <code>asyncio</code>","text":"<p><code>asyncio</code> is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: <code>OpenAI()</code> and <code>AsyncOpenAI()</code>. Today, we will be using the <code>AsyncOpenAI()</code> class, which processes data asynchronously.</p> <p>By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-async-and-await","title":"Understanding <code>async</code> and <code>await</code>","text":"<p>We will be using the <code>async</code> and <code>await</code> keywords to define asynchronous functions. The <code>async</code> keyword is used to define a function that returns a coroutine object. The <code>await</code> keyword is used to wait for the result of a coroutine object.</p> <p>If you want to understand the deeper details of <code>asyncio</code>, I recommend reading this article by Real Python.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-gather-vs-as_completed","title":"Understanding <code>gather</code> vs <code>as_completed</code>","text":"<p>In this post we'll show two ways to run tasks concurrently: <code>asyncio.gather</code> and <code>asyncio.as_completed</code>. The <code>gather</code> method is used to run multiple tasks concurrently and return the results as a <code>list</code>. The <code>as_completed</code> returns a <code>iterable</code> is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#example-batch-processing","title":"Example: Batch Processing","text":"<p>In this example, we will demonstrate how to use <code>asyncio</code> for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using <code>asyncio</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables `response_model` in `create` method\nclient = instructor.apatch(AsyncOpenAI()) # (1)!\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_person(text: str) -&gt; Person:\n    return await client.chat.completions.create( # (2)!\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": text},\n        ],\n        response_model=Person,\n    )\n</code></pre> <ol> <li>We use <code>instructor.apatch</code> to patch the <code>create</code> method of <code>AsyncOpenAI</code> to accept a <code>response_model</code> argument. This is because the <code>create</code> method of <code>AsyncOpenAI</code> does not accept a <code>response_model</code> argument without this patch.</li> <li>We use <code>await</code> here to wait for the response from the server before we return the result. This is because <code>create</code> returns a coroutine object, not the result of the coroutine.</li> </ol> <p>Notice that now there are <code>async</code> and <code>await</code> keywords in the function definition. This is because we're using the <code>asyncio</code> library to run the function concurrently. Now lets define a batch of texts to process.</p> <pre><code>dataset = [\n        \"My name is John and I am 20 years old\",\n        \"My name is Mary and I am 21 years old\",\n        \"My name is Bob and I am 22 years old\",\n        \"My name is Alice and I am 23 years old\",\n        \"My name is Jane and I am 24 years old\",\n        \"My name is Joe and I am 25 years old\",\n        \"My name is Jill and I am 26 years old\",\n    ]\n</code></pre>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#for-loop-running-tasks-sequentially","title":"<code>for loop</code>: Running tasks sequentially.","text":"<pre><code>persons = []\nfor text in dataset:\n    person = await extract_person(text)\n    persons.append(person)\n</code></pre> <p>Even though there is an <code>await</code> keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a <code>for</code> loop to iterate over the dataset. This method, which uses a <code>for</code> loop, will be the slowest among the four methods discussed today.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#asynciogather-running-tasks-concurrently","title":"<code>asyncio.gather</code>: Running tasks concurrently.","text":"<pre><code>async def gather():\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    all_persons = await asyncio.gather(*tasks_get_persons) # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for all the tasks to finish before assigning the result to <code>all_persons</code>. This is because <code>asyncio.gather</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.as_completed</code> to achieve the same result.</li> </ol> <p>Using <code>asyncio.gather</code> allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where <code>asyncio.as_completed</code> comes into play.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#asyncioas_completed-handling-tasks-as-they-complete","title":"<code>asyncio.as_completed</code>: Handling tasks as they complete.","text":"<pre><code>async def as_completed():\n    all_persons = []\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person) # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</li> </ol> <p>This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client.</p> <p>However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make.</p> <p>Ordering of results</p> <p>Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use <code>asyncio.gather</code> instead.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#rate-limited-gather-using-semaphores-to-limit-concurrency","title":"Rate-Limited Gather: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem: # (1)!\n        return await extract_person(text)\n\nasync def rate_limited_gather(sem: Semaphore):\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    resp = await asyncio.gather(*tasks_get_persons)\n</code></pre> <ol> <li>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</li> </ol>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#rate-limited-as-completed-using-semaphores-to-limit-concurrency","title":"Rate-Limited As Completed: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem: # (1)!\n        return await extract_person(text)\n\nasync def rate_limited_as_completed(sem: Semaphore):\n    all_persons = []\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person) # (2)!\n</code></pre> <ol> <li> <p>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</p> </li> <li> <p>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</p> </li> </ol> <p>Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced.</p> <p>Other Options</p> <p>Its important to also note that here we are using a <code>semaphore</code> to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the <code>openai</code> request. You can imagine using a library like <code>ratelimit</code> to limit the number of requests per second. OR catching rate limit exceptions and using <code>tenacity</code> to retry the request after a certain amount of time.</p> <ul> <li>tenacity</li> <li>aiolimiter</li> </ul>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#results","title":"Results","text":"<p>As you can see, the <code>for</code> loop is the slowest, while <code>asyncio.as_completed</code> and <code>asyncio.gather</code> are the fastest without any rate limiting.</p> Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as_completed 3.26 seconds 2","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#practical-implications-of-batch-processing","title":"Practical implications of batch processing","text":"<p>The choice of approach depends on the task's nature and the desired balance between speed and resource utilization.</p> <p>Here are some guidelines to consider:</p> <ul> <li>Use <code>asyncio.gather</code> for handling multiple independent tasks quickly.</li> <li>Apply <code>asyncio.as_completed</code> for large datasets to process tasks as they complete.</li> <li>Implement rate-limiting to avoid overwhelming servers or API endpoints.</li> </ul> <p>If you find the content helpful or want to try out <code>Instructor</code>, please visit our GitHub page and give us a star!</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/","title":"RAG is more than just embedding search","text":"<p>With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p> <p>What is RAG?</p> <p>Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p> <p> </p> Simple RAG that embedded the user query and makes a search. <p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model\u2014a basic setup that's more common than you'd think.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#the-dumb-rag-model","title":"The 'Dumb' RAG Model","text":"<p>When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#why-is-this-a-problem","title":"Why is this a problem?","text":"<ul> <li> <p>Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p> </li> <li> <p>Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p> </li> <li> <p>Limitation of text search: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking <code>what problems did we fix last week</code> cannot be answered by a simple text search since documents that contain <code>problem, last week</code> are going to be present at every week.</p> </li> <li> <p>Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results.</p> </li> </ul> <p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#improving-the-rag-model-with-query-understanding","title":"Improving the RAG Model with Query Understanding","text":"<p>Shoutouts</p> <p>Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out!</p> <p>Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall.</p> <p> </p> Query Understanding system routes to multiple search backends. <p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-instructor","title":"Whats instructor?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-1-metaphor-systems","title":"Case Study 1: Metaphor Systems","text":"<p>Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query.</p> <p></p> Metaphor Systems UI <p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n\nclass MetaphorQuery(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n    domains_allow_list: List[str]\n\n    async def execute():\n        return await metaphor.search(...)\n</code></pre> <p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nquery = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=MetaphorQuery,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What are some recent developments in AI?\"\n        }\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n  \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n  \"published_daterange\": {\n    \"start\": \"2023-09-17\",\n    \"end\": \"2021-06-17\"\n  },\n  \"domains_allow_list\": [\"arxiv.org\"]\n}\n</code></pre> <p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n    chain_of_thought: str = Field(\n        None,\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n</code></pre> <p>Now, let's see how this approach can help model an agent like personal assistant.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-2-personal-assistant","title":"Case Study 2: Personal Assistant","text":"<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p> <pre><code>class ClientSource(enum.Enum):\n    GMAIL = \"gmail\"\n    CALENDAR = \"calendar\"\n\nclass SearchClient(BaseModel):\n    query: str\n    keywords: List[str]\n    email: str\n    source: ClientSource\n    start_date: datetime.date\n    end_date: datetime.date\n\n    async def execute(self) -&gt; str:\n        if self.source == ClientSource.GMAIL:\n            ...\n        elif self.source == ClientSource.CALENDAR:\n            ...\n\nclass Retrival(BaseModel):\n    queries: List[SearchClient]\n\n    async def execute(self) -&gt; str:\n        return await asyncio.gather(*[query.execute() for query in self.queries])\n</code></pre> <p>Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nretrival = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Retrival,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"},\n        {\"role\": \"user\", \"content\": \"What do I have today?\"}\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n    \"queries\": [\n        {\n            \"query\": None,\n            \"keywords\": None,\n            \"email\": \"jason@example.com\",\n            \"source\": \"gmail\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n        },\n        {\n            \"query\": None,\n            \"keywords\": [\"meeting\", \"call\", \"zoom\"]]],\n            \"email\": \"jason@example.com\",\n            \"source\": \"calendar\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n\n        }\n    ]\n}\n</code></pre> <p>Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p> <p>Can I used framework X?</p> <p>I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit.</p> <p>Both of these examples showcase how both search providors and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#conclusion","title":"Conclusion","text":"<p>This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-next","title":"What's Next?","text":"<p>Here I want to show that `instructor`` isn\u2019t just about data extraction. It\u2019s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning \u2014 the untapped goldmine is skilled use of tools and APIs.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/","title":"Good LLM Validation is Just Good Validation","text":"<p>What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.</p> <p>Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like <code>Pydantic</code> and <code>Instructor</code>. We validate these outputs using a validation function which conforms to the structure seen below.</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#what-is-instructor","title":"What is Instructor?","text":"<p><code>Instructor</code> helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the <code>Pydantic</code> model for your desired response, <code>Instructor</code> handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai.</p> <pre><code>from openai import OpenAI\nimport instructor # pip install instructor\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI()) # (1)!\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n    max_retries=3 # (2)!\n)\n\nassert user.name == \"Jason\" # (3)!\nassert user.age == 25\n</code></pre> <ol> <li> <p>To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we     offer a patching mechanism for the <code>ChatCompletion</code> class.</p> </li> <li> <p>Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define.</p> </li> <li> <p>As long as you pass in a <code>response_model</code> parameter to the <code>ChatCompletion</code> api call, the returned object will always     be a validated <code>Pydantic</code> object.</p> </li> </ol> <p>In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use <code>Pydantic</code> and <code>Instructor</code> to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation.</p> <p>Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-10-introduction-to-validations-in-pydantic","title":"Software 1.0: Introduction to Validations in Pydantic","text":"<p>A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words <code>Steal</code> and <code>Rob</code> are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this.</p> <p>This will throw an error if we pass in a string like <code>Let's rob the bank!</code> or <code>We should steal from the supermarkets</code>.</p> <p>Pydantic offers two approaches for this validation: using the <code>field_validator</code> decorator or the <code>Annotated</code> hints.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-field_validator-decorator","title":"Using <code>field_validator</code> decorator","text":"<p>We can use the <code>field_validator</code> decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so.</p> <pre><code>from pydantic import BaseModel, ValidationError, field_validator\nfrom pydantic.fields import Field\n\nclass UserMessage(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:\n        for word in v.split(): # (1)!\n            if word.lower() in {'rob','steal'}:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <ol> <li>We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these     words are in our blacklist which in this case is just <code>rob</code> and <code>steal</code></li> </ol> <p>Since the message <code>This is a lovely day</code> does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message <code>We should go and rob a bank</code> due to the presence of the word <code>rob</code> and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>Alternatively, you can use the <code>Annotated</code> function to perform the same validation. Here's an example where we utilise the same function we started with.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\ndef message_cannot_have_blacklisted_words(value:str):\n    for word in value.split():\n        if word.lower() in {'rob','steal'}:\n            raise ValueError(f\"`{word}` was found in the message `{value}`\")\n    return value\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a <code>ValueError</code> is raised and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre> <p>Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged.</p> <p>Suppose now that we've gotten a new message - <code>Violence is always acceptable, as long as we silence the witness</code>. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words <code>rob</code> or <code>steal</code>. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges?</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-30-validation-for-llms-or-powered-by-llms","title":"Software 3.0: Validation for LLMs or powered by LLMs","text":"<p>Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called <code>llm_validator</code> that uses a statement to verify the value.</p> <p>We can get around this by using the inbuilt <code>llm_validator</code> class from <code>Instructor</code>.</p> <pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(llm_validator(\"don't say objectionable things\"))]\n\ntry:\n    UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>This produces the following error message as seen below</p> <pre><code>1 validation error for UserMessage\nmessage\n  Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error\n</code></pre> <p>The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an <code>llm_validator</code> from scratch.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#creating-your-own-field-level-llm_validator","title":"Creating Your Own Field Level <code>llm_validator</code>","text":"<p>Building your own <code>llm_validator</code> can be a valuable exercise to get started with <code>Instructor</code> and create custom validators.</p> <p>Before we continue, let's review the anatomy of a validator:</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return value\n</code></pre> <p>As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a <code>ValueError</code>. We can represent this using the following structure:</p> <pre><code>class Validation(BaseModel):\n    is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")\n    error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n</code></pre> <p>Using this structure, we can implement the same logic as before and utilize <code>Instructor</code> to generate the validation.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.patch(OpenAI())\n\ndef validator(v):\n    statement = \"don't say objectionable things\"\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Does `{v}` follow the rules: {statement}\",\n            },\n        ],\n        # this comes from client = instructor.patch(OpenAI())\n        response_model=Validation, # (1)!\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return v\n</code></pre> <ol> <li>The new parameter of <code>response_model</code> comes from <code>client = instructor.patch(OpenAI())</code> and does not exist in the original OpenAI SDK. This    allows us to pass in the <code>Pydantic</code> model that we want as a response.</li> </ol> <p>Now we can use this validator in the same way we used the <code>llm_validator</code> from <code>Instructor</code>.</p> <pre><code>class UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(validator)]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#writing-more-complex-validations","title":"Writing more complex validations","text":"","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-chain-of-thought","title":"Validating Chain of Thought","text":"<p>A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt.</p> <p>We can utilise <code>Pydantic</code> and <code>Instructor</code> to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator.</p> <pre><code>def validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        # this comes from client = instructor.patch(OpenAI())\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n</code></pre> <p>We can then take advantage of the <code>model_validator</code> decorator to perform a validation on a subset of the model's data.</p> <p>We're defining a model validator here which runs before <code>Pydantic</code> parses the input into its respective fields. That's why we have a before keyword used in the <code>model_validator</code> class.</p> <pre><code>from pydantic import BaseModel, model_validator\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n</code></pre> <p>Now, when you create a <code>AIResponse</code> instance, the <code>chain_of_thought_makes_sense</code> validator will be invoked. Here's an example:</p> <pre><code>try:\n    resp = AIResponse(\n        chain_of_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\"\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>If we create a <code>AIResponse</code> instance with an answer that does not follow the chain of thought, we will get an error.</p> <pre><code>1 validation error for AIResponse\n    Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2.\n    [type=value_error, input_value={'chain_of_thought': '1 +... meaning of life is 42'}, input_type=dict]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-citations-from-original-text","title":"Validating Citations From Original Text","text":"<p>Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically.</p> <p>We can pass in additional context to our validation functions using the <code>model_validate</code> function in <code>Pydantic</code> so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the <code>info</code> argument in our validator functions.</p> <pre><code>from pydantic import ValidationInfo,BaseModel,field_validator\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo): # (1)!\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text chunks\")\n        return v\n</code></pre> <ol> <li>This <code>info</code> object corresponds to the value of <code>context</code> that we pass into the <code>model_validate</code> function as seen below.</li> </ol> <p>We can then take our original example and test it against our new model</p> <pre><code>try:\n    AnswerWithCitation.model_validate(\n        {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"},\n        context={\"text_chunk\": \"Jason is just a guy\"}, # (1)!\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <ol> <li>This <code>context</code> object is just a normal python dictionary and can take in and store any arbitrary values</li> </ol> <p>This in turn generates the following error since <code>Jason is cool</code> does not exist in the text <code>Jason is just a guy</code>.</p> <pre><code>1 validation error for AnswerWithCitation\ncitation\nValue error, Citation `Jason is cool` not found in text chunks [type=value_error, input_value='Jason is cool', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#putting-it-all-together-with-client-instructorpatchopenai","title":"Putting it all together with <code>client = instructor.patch(OpenAI())</code>","text":"<p>To pass this context from the <code>client.chat.completions.create</code> call, <code>client = instructor.patch(OpenAI())</code> also passes the <code>validation_context</code>, which will be accessible from the <code>info</code> argument in the decorated validator functions.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.patch(OpenAI())\n\ndef answer_question(question:str, text_chunk: str) -&gt; AnswerWithCitation:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Answer the question: {question} with the text chunk: {text_chunk}\",\n            },\n        ],\n        response_model=AnswerWithCitation,\n        validation_context={\"text_chunk\": text_chunk},\n    )\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#error-handling-and-re-asking","title":"Error Handling and Re-Asking","text":"<p>Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running <code>client = instructor.patch(OpenAI())</code> not only do we add <code>response_model</code> and <code>validation_context</code> it also allows you to use the <code>max_retries</code> parameter to specify the number of times to try and self correct.</p> <p>This approach provides a layer of defense against two types of bad outputs:</p> <ol> <li>Pydantic Validation Errors (code or LLM-based)</li> <li>JSON Decoding Errors (when the model returns an incorrect response)</li> </ol>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#define-the-response-model-with-validators","title":"Define the Response Model with Validators","text":"<p>To keep things simple lets assume we have a model that returns a <code>UserModel</code> object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase.</p> <pre><code>from pydantic import BaseModel, field_validator\n\nclass UserModel(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre> <p>This is where the <code>max_retries</code> parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt.</p> <pre><code>model = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n    # Powered by client = instructor.patch(OpenAI())\n    response_model=UserModel,\n    max_retries=2,\n)\n\nassert model.name == \"JASON\"\n</code></pre> <p>In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#conclusion","title":"Conclusion","text":"<p>From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it.</p> <p>If you enjoy the content or want to try out <code>Instructor</code> please check out the github and give us a star!</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"cli/","title":"Instructor CLI","text":"<p>Welcome to the Instructor Command-Line Interface (CLI), a tool designed to ease your experience with the OpenAI API. Whether it's tracking your API usage or fine-tuning your models, Instructor CLI is your go-to utility.</p>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<p>First things first: make sure your OpenAI API key is set as an environment variable. The CLI will use this for authenticating your requests to OpenAI's services.</p> <p>You can set the API key in your terminal as follows:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"cli/#installation-setup","title":"Installation &amp; Setup","text":"<pre><code>pip install instructor\n</code></pre>"},{"location":"cli/#features","title":"Features","text":"<ul> <li> <p>API Usage Monitoring: Keep tabs on your API usage right from the terminal. Track token counts, total requests, and even calculate the costs. To learn more, consult the Usage Guide.</p> </li> <li> <p>Model Fine-Tuning: Optimize your models to meet your specific requirements using our fine-tuning app. For more details, check out the Fine-Tuning Guide.</p> </li> </ul>"},{"location":"cli/#support-contribution","title":"Support &amp; Contribution","text":"<p>Need help or want to contribute? Visit our GitHub Repository</p>"},{"location":"cli/finetune/","title":"Using the Command Line Interface","text":"<p>The instructor CLI provides functionalities for managing fine-tuning jobs on OpenAI.</p> <p>Incomplete API</p> <p>The CLI is still under development and does not yet support all features of the API. If you would like to use a feature that is not yet supported, please consider using the contributing to our library jxnl/instructor instead.</p> <p>Low hanging fruit</p> <p>If you want to contribute we're looking for a few things:</p> <ol> <li>Adding filenames on upload</li> </ol>"},{"location":"cli/finetune/#creating-a-fine-tuning-job","title":"Creating a Fine-Tuning Job","text":""},{"location":"cli/finetune/#view-jobs-options","title":"View Jobs Options","text":"<pre><code>$ instructor jobs --help \n\n Usage: instructor jobs [OPTIONS] COMMAND [ARGS]...                                                            \n\n Monitor and create fine tuning jobs                                                                           \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help                            Display the help message.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 cancel                    Cancel a fine-tuning job.                                                         \u2502\n\u2502 create-from-file          Create a fine-tuning job from a file.                                             \u2502\n\u2502 create-from-id            Create a fine-tuning job from an existing ID.                                     \u2502\n\u2502 list                      Monitor the status of the most recent fine-tuning jobs.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#create-from-file","title":"Create from File","text":"<p>The create-from-file command uploads and trains a model in a single step.</p> <pre><code>\u276f instructor jobs create-from-file --help\n\nUsage: instructor jobs create-from-file [OPTIONS] FILE                                              \n\n Create a fine-tuning job from a file.                                                               \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    file      TEXT  Path to the file for fine-tuning [default: None] [required]                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning [default: gpt-3.5-turbo]  \u2502\n\u2502 --poll                            INTEGER  Polling interval in seconds [default: 2]               \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning                       \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                             \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning               \u2502\n\u2502 --validation-file                 TEXT     Path to the validation file [default: None]            \u2502\n\u2502 --model-suffix                    TEXT     Suffix to identify the model [default: None]           \u2502\n\u2502 --help                                     Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"cli/finetune/#usage","title":"Usage","text":"<pre><code>$ instructor jobs create-from-file transformed_data.jsonl --validation_file validation_data.jsonl --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#create-from-id","title":"Create from ID","text":"<p>The create-from-id command uses an uploaded file and trains a model</p> <pre><code>\u276f instructor jobs create-from-id --help\n\n Usage: instructor jobs create-from-id [OPTIONS] ID                                      \n\n Create a fine-tuning job from an existing ID.                                           \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    id      TEXT  ID of the existing fine-tuning job [default: None] [required]      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning               \u2502\n\u2502                                            [default: gpt-3.5-turbo]                   \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning           \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                 \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning   \u2502\n\u2502 --validation-file-id              TEXT     ID of the uploaded validation file         \u2502\n\u2502                                            [default: None]                            \u2502\n\u2502 --help                                     Show this message and exit.                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#usage_1","title":"Usage","text":"<pre><code>$ instructor files upload transformed_data.jsonl\n$ instructor files upload validation_data.jsonl \n$ instructor files list\n...\n$ instructor jobs create_from_id &lt;file_id&gt; --validation_file &lt;validation_file_id&gt; --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#viewing-files-and-jobs","title":"Viewing Files and Jobs","text":""},{"location":"cli/finetune/#viewing-jobs","title":"Viewing Jobs","text":"<pre><code>$ instructor jobs list \n\nOpenAI Fine Tuning Job Monitoring                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                \u2503              \u2503                \u2503     Completion \u2503                 \u2503                \u2503        \u2503                 \u2503\n\u2503 Job ID         \u2503 Status       \u2503  Creation Time \u2503           Time \u2503 Model Name      \u2503 File ID        \u2503 Epochs \u2503 Base Model      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 ftjob-PWo6uwk\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       23:10:54 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-1whjva8\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:47:05 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-wGoBDld\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:44:12 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-yd5aRTc\u2026 \u2502 \u2705 succeeded \u2502     2023-08-23 \u2502     2023-08-23 \u2502 ft:gpt-3.5-tur\u2026 \u2502 file-IQxAUDqX\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       14:26:03 \u2502       15:02:29 \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    Automatically refreshes every 5 seconds, press Ctrl+C to exit\n</code></pre>"},{"location":"cli/finetune/#viewing-files","title":"Viewing Files","text":"<pre><code>$ instructor files list \n\nOpenAI Files                                                      \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513                         \n\u2503 File ID                       \u2503 Size (bytes) \u2503 Creation Time       \u2503 Filename \u2503 Purpose   \u2503                         \n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529                         \n\u2502 file-0lw2BSNRUlXZXRRu2beCCWjl \u2502       369523 \u2502 2023-08-23 23:31:57 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-IHaUXcMEykmFUp1kt2puCDEq \u2502       369523 \u2502 2023-08-23 23:09:35 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-ja9vRBf0FydEOTolaa3BMqES \u2502       369523 \u2502 2023-08-23 22:42:29 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-F7lJg6Z47CREvmx4kyvyZ6Sn \u2502       369523 \u2502 2023-08-23 22:42:03 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-YUxqZPyJRl5GJCUTw3cNmA46 \u2502       369523 \u2502 2023-08-23 22:29:10 \u2502 file     \u2502 fine-tune \u2502                         \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \n</code></pre>"},{"location":"cli/finetune/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"cli/usage/","title":"Using the OpenAI API Usage CLI","text":"<p>The OpenAI API Usage CLI tool provides functionalities for monitoring your OpenAI API usage, breaking it down by model, date, and cost.</p>"},{"location":"cli/usage/#monitoring-api-usage","title":"Monitoring API Usage","text":""},{"location":"cli/usage/#view-usage-options","title":"View Usage Options","text":"<pre><code>$ instructor usage --help\n\n Usage: instructor usage [OPTIONS] COMMAND [ARGS]...                                                           \n\n Check OpenAI API usage data                                                                                   \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 list       Displays OpenAI API usage data for the past N days.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/usage/#list-usage-for-specific-number-of-days","title":"List Usage for Specific Number of Days","text":"<p>To display API usage for the past 3 days, use the following command:</p> <pre><code>$ instructor usage list -n 3\n</code></pre> <p>This will output a table similar to:</p> <pre><code>                 Usage Summary by Date, Snapshot, and Cost\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Date       \u2503 Snapshot ID               \u2503 Total Requests \u2503 Total Cost ($) \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2023-09-04 \u2502 gpt-4-0613                \u2502             44 \u2502           0.68 \u2502\n\u2502 2023-09-04 \u2502 gpt-3.5-turbo-16k-0613    \u2502            195 \u2502           0.84 \u2502\n\u2502 2023-09-04 \u2502 text-embedding-ada-002-v2 \u2502            276 \u2502           0.00 \u2502\n\u2502 2023-09-04 \u2502 gpt-4-32k-0613            \u2502            328 \u2502          49.45 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/usage/#list-usage-for-today","title":"List Usage for Today","text":"<p>To display the API usage for today, simply run:</p> <pre><code>$ instructor usage list\n</code></pre>"},{"location":"cli/usage/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"concepts/alias/","title":"Alias","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/caching/","title":"Caching","text":"<p>If you want to learn more about concepts in caching and how to use them in your own projects, check out our blog on the topic.</p>"},{"location":"concepts/caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import functools\nimport instructor\n\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@functools.cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <pre><code>import time\n\nstart = time.perf_counter() # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\") # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\n&gt;&gt;&gt; Time taken: 0.9267581660533324\n&gt;&gt;&gt; Time taken: 1.2080417945981026e-06 # (3)\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> <li>The second call to <code>extract</code> is much faster because the result is returned from the cache!</li> </ol> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\") # (1)\n        result = func(*args, **kwargs)\n        print(\"Do something after\") # (2)\n        return result\n    return wrapper\n\n@decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n&gt;&gt;&gt; \"Do something before\"\n&gt;&gt;&gt; \"Hello!\"\n&gt;&gt;&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>"},{"location":"concepts/caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory') # (1)\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel): # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation # (4)\n    if not issubclass(return_type, BaseModel): # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\" #  (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>"},{"location":"concepts/caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport json\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel): # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\" # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementation is the same, but we're using a different caching backend!</p>"},{"location":"concepts/distillation/","title":"Distilling python functions into LLM","text":"<p><code>Instructions</code> from the <code>Instructor</code> library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning <code>gpt-3.5-turbo</code> to emulate these functions end-to-end.</p> <p>If you want to see the full example checkout examples/distillation</p>"},{"location":"concepts/distillation/#the-challenges-in-function-level-fine-tuning","title":"The Challenges in Function-Level Fine-Tuning","text":"<p>Replicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing <code>def f(a, b): return a * b</code>. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens.</p>"},{"location":"concepts/distillation/#the-role-of-instructions-in-simplifying-the-fine-tuning-process","title":"The Role of <code>Instructions</code> in Simplifying the Fine-Tuning Process","text":"<p>By using <code>Instructions</code>, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset.</p>"},{"location":"concepts/distillation/#how-to-implement-instructions-in-your-code","title":"How to Implement <code>Instructions</code> in Your Code","text":""},{"location":"concepts/distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")]\n)\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n# Generate some data\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n</code></pre>"},{"location":"concepts/distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>"},{"location":"concepts/distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>"},{"location":"concepts/distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>"},{"location":"concepts/distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\"role\": \"assistant\",\n            \"function_call\":\n                {\n                    \"name\": \"Multiply\",\n                    \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}'\n            }\n        }\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ]\n}\n</code></pre> <p>Run a finetune like this:</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre> <p>Once a model is trained you can simply change <code>mode</code> to <code>dispatch</code> and it will use the model to run the function!</p> <pre><code>from instructor import Instructions\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -&gt; Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>"},{"location":"concepts/enums/","title":"Enums","text":"<p>To prevent data misalignment, we can use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from enum import Enum, auto\n\nclass Role(Enum):\n    PRINCIPAL = \"PRINCIPAL\"\n    TEACHER = \"TEACHER\"\n    STUDENT = \"STUDENT\"\n    OTHER = \"OTHER\"\"\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(description=\"Correctly assign one of the predefined roles to the user.\")\n</code></pre> <p>If you're having a hard time with <code>Enum</code> and alternative is to use <code>Literal</code> instead.</p> <pre><code>class UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre>"},{"location":"concepts/fastapi/","title":"Integrating Pydantic Models with FastAPI","text":"<p>FastAPI is an enjoyable tool for building web applications in Python. It is well known for its integration with <code>Pydantic</code> models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return <code>Pydantic</code> models can seamlessly integrate with <code>FastAPI</code>.</p>"},{"location":"concepts/fastapi/#why-choose-fastapi-and-pydantic","title":"Why Choose FastAPI and Pydantic?","text":"<ul> <li>FastAPI is a modern, high-performance web framework for building APIs with Python.</li> <li>Supports OpenAPI and JSON Schema for automatic documentation and validation.</li> <li>Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client</li> </ul>"},{"location":"concepts/fastapi/#code-example-starting-a-fastapi-app-with-a-post-request","title":"Code Example: Starting a FastAPI App with a POST Request","text":"<p>The following code snippet demonstrates how to start a <code>FastAPI</code> app with a POST endpoint. This endpoint accepts and returns data defined by a <code>Pydantic</code> model.</p> <pre><code>import instructor\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables response_model\nclient = instructor.patch(AsyncOpenAI())\napp = FastAPI()\n\nclass UserData(BaseModel):\n    # This can be the model for the input data\n    query: str\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@app.post(\"/endpoint\", response_model=UserDetail)\ndef endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ]\n    )\n    return user_detail\n</code></pre>"},{"location":"concepts/fastapi/#streaming-responses-with-fastapi","title":"Streaming Responses with FastAPI","text":"<p><code>FastAPI</code> supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data.</p> <pre><code>from fastapi import StreamingResponse\n\n# Route to handle SSE events and return users\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ]\n    )\n\n    async def generate():\n        for user in users:\n            resp_json = user.model_dump_json()\n            yield f\"data: {resp_json}\"\n        yield \"data: [DONE]\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre>"},{"location":"concepts/fastapi/#automatic-documentation-with-fastapi","title":"Automatic Documentation with FastAPI","text":"<p>FastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the <code>/docs</code> page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser.</p> <p>To explore the capabilities of your API, follow these steps:</p> <ol> <li>Run the API using the Uvicorn command: <code>uvicorn main:app --reload</code>.</li> <li>Open your web browser and navigate to <code>http://127.0.0.1:8000/docs</code>.</li> <li>You will find an interactive UI where you can send different requests to your API and see the responses in real-time.</li> </ol> <p></p>"},{"location":"concepts/fields/","title":"Fields","text":"<p>The <code>pydantic.Field</code> function is used to customize and add metadata to fields of models. To learn more check out the pydantic documentation as this is a near replica of that documentation that is relevant to prompting.</p>"},{"location":"concepts/fields/#default-values","title":"Default values","text":"<p>The <code>default</code> parameter is used to define a default value for a field.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(default='John Doe')\n\n\nuser = User()\nprint(user)\n#&gt; name='John Doe'\n</code></pre> <p>You can also use <code>default_factory</code> to define a callable that will be called to generate a default value.</p> <pre><code>from uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n</code></pre> <p>Info</p> <p>The <code>default</code> and <code>default_factory</code> parameters are mutually exclusive.</p> <p>Note</p> <p>If you use <code>typing.Optional</code>, it doesn't mean that the field has a default value of <code>None</code> you must use <code>default</code> or <code>default_factory</code> to define a default value. Then it will be considered <code>not required</code> when sent to the language model.</p>"},{"location":"concepts/fields/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>The <code>Field</code> function can also be used together with <code>Annotated</code>.</p> <pre><code>from uuid import uuid4\n\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n</code></pre>"},{"location":"concepts/fields/#exclude","title":"Exclude","text":"<p>The <code>exclude</code> parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like <code>scratch_pad</code> or <code>chain_of_thought</code></p> <p>See the following example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom datetime import date\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Reasoning behind the date range.\"\n        exclude=True)\n    start_date: date\n    end_date: date\n\n\ndate_range = DateRange(\n    chain_of_thought=\"\"\"\n        I want to find the date range for the last 30 days.\n        Today is 2021-01-30 therefore the start date\n        should be 2021-01-01 and the end date is 2021-01-30\"\"\",\n    start_date=date(2021, 1, 1),\n    end_date=date(2021, 1, 30),\n)\nprint(date_range.model_dump_json())\n#&gt; start_date=datetime.date(2021, 1, 1) end_date=datetime.date(2021, 1, 30)\n</code></pre>"},{"location":"concepts/fields/#customizing-json-schema","title":"Customizing JSON Schema","text":"<p>There are fields that exclusively to customise the generated JSON Schema:</p> <ul> <li><code>title</code>: The title of the field.</li> <li><code>description</code>: The description of the field.</li> <li><code>examples</code>: The examples of the field.</li> <li><code>json_schema_extra</code>: Extra JSON Schema properties to be added to the field.</li> </ul> <p>These all work as great opportunities to add more information to the JSON Schema as part of your prompt engineering.</p> <p>Here's an example:</p> <pre><code>from pydantic import BaseModel, EmailStr, Field, SecretStr\n\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    email: EmailStr = Field(examples=['marcelo@mail.com'])\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\n\nprint(User.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'age': {\n            'description': 'Age of the user',\n            'title': 'Age',\n            'type': 'integer',\n        },\n        'email': {\n            'examples': ['marcelo@mail.com'],\n            'format': 'email',\n            'title': 'Email',\n            'type': 'string',\n        },\n        'name': {'title': 'Username', 'type': 'string'},\n        'password': {\n            'description': 'Password of the user',\n            'examples': ['123456'],\n            'format': 'password',\n            'title': 'Password',\n            'type': 'string',\n            'writeOnly': True,\n        },\n    },\n    'required': ['age', 'email', 'name', 'password'],\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/fields/#general-notes-on-json-schema-generation","title":"General notes on JSON schema generation","text":"<ul> <li>The JSON schema for Optional fields indicates that the value null is allowed.</li> <li>The Decimal type is exposed in JSON schema (and serialized) as a string.</li> <li>The JSON schema does not preserve namedtuples as namedtuples.</li> <li>When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization.</li> <li>Sub-models used are added to the <code>$defs</code> JSON attribute and referenced, as per the spec.</li> <li>Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced.</li> <li>The description for models is taken from either the docstring of the class or the argument description to the Field class.</li> </ul>"},{"location":"concepts/lists/","title":"Multi-task and Streaming","text":"<p>A common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction</p> <pre><code>from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclass Users(BaseModel):\n    users: List[User]\n</code></pre> <p>Defining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of <code>Iterable[T]</code>. This lets us dynamically create a new class that:</p> <ol> <li>Has dynamic docstrings and class name based on the task</li> <li>Support streaming by collecting tokens until a task is received back out.</li> </ol>"},{"location":"concepts/lists/#extracting-tasks-using-iterable","title":"Extracting Tasks using Iterable","text":"<p>By using <code>Iterable</code> you get a very convient class with prompts and names automatically defined:</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nUsers = Iterable[User]\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Users,\n    stream=False,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\n                         Correctly segment it into entitites\\\n                        Make sure the JSON is correct\",\n        },\n    ],\n)\nfor user in users:\n    assert isinstance(user, User)\n    print(user)\n\n&gt;&gt;&gt; name=\"Jason\" \"age\"=10\n&gt;&gt;&gt; name=\"John\" \"age\"=10\n</code></pre>"},{"location":"concepts/lists/#streaming-tasks","title":"Streaming Tasks","text":"<p>We can also generate tasks as the tokens are streamed in by defining an <code>Iterable[T]</code> type.</p> <p>Lets look at an example in action with the same class</p> <pre><code>from typing import Iterable\n\nUsers = Iterable[User]\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Users,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Consider the data below:\\n{input}\"\n                \"Correctly segment it into entitites\"\n                \"Make sure the JSON is correct\"\n            ),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    assert isinstance(user, User)\n    print(user)\n\n&gt;&gt;&gt; name=\"Jason\" \"age\"=10\n&gt;&gt;&gt; name=\"John\" \"age\"=10\n</code></pre> <p>This streaming is still a prototype, but should work quite well for simple schemas.</p>"},{"location":"concepts/maybe/","title":"Handling Missing Data","text":"<p>The <code>Maybe</code> pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning <code>None</code>, you can use a <code>Maybe</code> type to encapsulate both the result and potential errors. This pattern is particularly useful when making llm calls, as providing language models with an escape hatch can effectively reduce hallucinations.</p>"},{"location":"concepts/maybe/#defining-the-model","title":"Defining the Model","text":"<p>Using Pydantic, we'll first define the <code>UserDetail</code> and <code>MaybeUser</code> classes.</p> <pre><code>from pydantic import BaseModel, Field, Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Notice that <code>MaybeUser</code> has a <code>result</code> field that is an optional <code>UserDetail</code> instance where the extracted data will be stored. The <code>error</code> field is a boolean that indicates whether an error occurred, and the <code>message</code> field is an optional string that contains the error message.</p>"},{"location":"concepts/maybe/#defining-the-function","title":"Defining the function","text":"<p>Once we have the model defined, we can create a function that uses the <code>Maybe</code> pattern to extract the data.</p> <pre><code>import random\nimport instructor\nfrom openai import OpenAI\nfrom typing import Optional\n\n# This enables the `response_model` keyword\nclient = instructor.patch(OpenAI())\n\ndef extract(content: str) -&gt; MaybeUser:\n    return openai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeUser,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n\nuser1 = extract(\"Jason is a 25-year-old scientist\")\n# output:\n{\n  \"result\": {\n    \"age\": 25,\n    \"name\": \"Jason\",\n    \"role\": \"scientist\"\n  },\n  \"error\": false,\n  \"message\": null\n}\n\nuser2 = extract(\"Unknown user\")\n# output:\n{\n  \"result\": null,\n  \"error\": true,\n  \"message\": \"User not found\"\n}\n</code></pre> <p>As you can see, when the data is extracted successfully, the <code>result</code> field contains the <code>UserDetail</code> instance. When an error occurs, the <code>error</code> field is set to <code>True</code>, and the <code>message</code> field contains the error message.</p>"},{"location":"concepts/maybe/#handling-the-result","title":"Handling the result","text":"<p>There are a few ways we can handle the result. Normally, we can just access the individual fields.</p> <pre><code>def process_user_detail(maybe_user: MaybeUser):\n  if not maybe_user.error:\n      user = maybe_user.result\n      print(f\"User {user.name} is {user.age} years old\")\n  else:\n      print(f\"Not found: {user1.message}\")\n</code></pre>"},{"location":"concepts/maybe/#pattern-matching","title":"Pattern Matching","text":"<p>We can also use pattern matching to handle the result. This is a great way to handle errors in a structured way.</p> <pre><code>def process_user_detail(maybe_user: MaybeUser):\n    match maybe_user:\n        case MaybeUser(error=True, message=msg):\n            print(f\"Error: {msg}\")\n        case MaybeUser(result=user_detail) if user_detail:\n            assert isinstance(user_detail, UserDetail)\n            print(f\"User {user_detail.name} is {user_detail.age} years old\")\n        case _:\n            print(\"Unknown error\")\n</code></pre> <p>If you want to learn more about pattern matching, check out Pydantic's docs on Structural Pattern Matching</p>"},{"location":"concepts/models/","title":"Response Model","text":"<p>Defining llm output schemas in Pydantic is done via <code>pydantic.BaseModel</code>. To learn more about models in pydantic checkout their documentation.</p> <p>After defining a pydantic model, we can use it as as the <code>response_model</code> in your client <code>create</code> calls to openai. The job of the <code>response_model</code> is to define the schema and prompts for the language model and validate the response from the API and return a pydantic model instance.</p>"},{"location":"concepts/models/#prompting","title":"Prompting","text":"<p>When defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    \"\"\"\n    This is the prompt that will be used to generate the response.\n    Any instructions here will be passed to the language model.\n    \"\"\"\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n</code></pre> <p>Here all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the <code>create</code> method of the client and will be used to generate the response.</p>"},{"location":"concepts/models/#optional-values","title":"Optional Values","text":"<p>If we use <code>Optional</code> and <code>default</code> they will be considered not required when sent to the language model</p> <pre><code>class User(BaseModel):\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n    email: Optional[str] = Field(description=\"The email of the user.\", default=None)\n</code></pre>"},{"location":"concepts/models/#dynamic-model-creation","title":"Dynamic model creation","text":"<p>There are some occasions where it is desirable to create a model using runtime information to specify the fields. For this Pydantic provides the create_model function to allow models to be created on the fly:</p> <pre><code>from pydantic import BaseModel, create_model\n\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\nprint(BarModel)\n#&gt; &lt;class '__main__.BarModel'&gt;\nprint(BarModel.model_fields.keys())\n#&gt; dict_keys(['foo', 'bar', 'apple', 'banana'])\n</code></pre> When would I use this? <p>Consider a situation where the model is dynamically defined, based on some configuration or database. For example, we could have a database table that stores the properties of a model for some model name or id. We could then query the database for the properties of the model and use that to create the model.</p> <pre><code>SELECT property_name, property_type, description\nFROM prompt\nWHERE model_name = {model_name}\n</code></pre> <p>We can then use this information to create the model.</p> <pre><code>types = {\n    'string': str,\n    'integer': int,\n    'boolean': bool,\n    'number': float,\n    'List[str]': List[str],\n}\n\nBarModel = create_model(\n    'User',\n    **{\n        property_name: (types[property_type], description)\n        for property_name, property_type, description in cursor.fetchall()\n    },\n    __base__=BaseModel,\n)\n</code></pre> <p>This would be useful when different users have different descriptions for the same model. We can use the same model but have different prompts for each user.</p>"},{"location":"concepts/models/#structural-pattern-matching","title":"Structural Pattern Matching","text":"<p>Pydantic supports structural pattern matching for models, as introduced by PEP 636 in Python 3.10.</p> <pre><code>from pydantic import BaseModel\n\n\nclass Pet(BaseModel):\n    name: str\n    species: str\n\n\na = Pet(name='Bones', species='dog')\n\nmatch a:\n    # match `species` to 'dog', declare and initialize `dog_name`\n    case Pet(species='dog', name=dog_name):\n        print(f'{dog_name} is a dog')\n        #&gt; Bones is a dog\n    # default case\n    case _:\n        print('No dog matched')\n</code></pre>"},{"location":"concepts/models/#adding-behavior","title":"Adding Behavior","text":"<p>We can add methods to our pydantic models just as any plain python class. We might want to do this to add some custom logic to our models.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nfrom openai import OpenAI\n\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\nclass SearchQuery(BaseModel):\n    query: str\n    query_type: Literal[\"web\", \"image\", \"video\"]\n\n    def execute(self):\n        # do some logic here\n        return results\n\n\nquery = client.chat.completions.create(\n        ..., response_model=SearchQuery\n    )\n\nresults = query.execute()\n</code></pre> <p>Now we can call <code>execute</code> on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on RAG is more than embeddings</p>"},{"location":"concepts/patching/","title":"Patching","text":"<p>Instructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits.</p> <ul> <li><code>response_model</code>: Defines the response type for <code>chat.completions.create</code>.</li> <li><code>max_retries</code>: Determines retry attempts for failed <code>chat.completions.create</code> validations.</li> <li><code>validation_context</code>: Provides extra context to the validation process.</li> </ul> <p>There are three methods for structured output:</p> <ol> <li>Function Calling: The primary method. Use this for stability and testing.</li> <li>Tool Calling: Useful in specific scenarios; lacks the reasking feature of OpenAI's tool calling API.</li> <li>JSON Mode: Offers closer adherence to JSON but with more potential validation errors. Suitable for specific non-function calling clients.</li> </ol>"},{"location":"concepts/patching/#function-calling","title":"Function Calling","text":"<pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI())\n</code></pre>"},{"location":"concepts/patching/#tool-calling","title":"Tool Calling","text":"<pre><code>import instructor\nfrom instructor import Mode\n\nclient = instructor.patch(OpenAI(), mode=Mode.TOOLS)\n</code></pre>"},{"location":"concepts/patching/#json-mode","title":"JSON Mode","text":"<pre><code>import instructor\nfrom instructor import Mode\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=Mode.JSON)\n</code></pre>"},{"location":"concepts/patching/#schema-integration","title":"Schema Integration","text":"<p>In JSON Mode, the schema is part of the system message:</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"Match your response to this json_schema: \\n{UserExtract.model_json_schema()['properties']}\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract jason is 25 years old\",\n        },\n    ],\n)\nuser = UserExtract.from_response(response, mode=Mode.JSON)\nassert user.name.lower() == \"jason\"\nassert user.age == 25\n</code></pre>"},{"location":"concepts/philosophy/","title":"Philosophy","text":"<p>The instructor values simplicity and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let Pydantic do the heavy lifting.</p> <p>\u201cSimplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"concepts/philosophy/#the-bridge-to-object-oriented-programming","title":"The Bridge to Object-Oriented Programming","text":"<p><code>instructor</code> acts as a bridge converting text-based LLM interactions into a familiar object-oriented format. Its integration with Pydantic provides type hints, runtime validation, and robust IDE support; love and supported by many in the Python ecosystem. By treating LLMs as callable functions returning typed objects, instructor makes language models backwards compatible with code, making them practical for everyday use while being complex enough for advanced applications.</p>"},{"location":"concepts/philosophy/#the-zen-of-instructor","title":"The zen of <code>instructor</code>","text":"<p>Maintain the flexibility and power of Python, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. With my experience maintaining a large enterprize framework at my previous job over many years I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define a Schema <code>class StructuredData(BaseModel):</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>def extract(a) -&gt; StructuredData:</code></li> <li>Define typed computations against your data with <code>def compute(data: StructuredData):</code> or call methods on your schema <code>data.compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"concepts/philosophy/#my-goals","title":"My Goals","text":"<p>The goal for the library, documentation, and blog, is to help you be a better python programmer and as a result a better AI engineer.</p> <ul> <li>The library is a result of my desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>I won't try to write prompts for you,</li> <li>I don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on twitter</p> <p>Cheers!</p>"},{"location":"concepts/prompting/","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use Pydantic's <code>Field</code> for clear field descriptions.</li> <li>Optionality: Use Python's <code>Optional</code> type for nullable fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"concepts/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(...,\n        description=\"Think step by step to determine the correct title\")\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#utilize-optional-attributes","title":"Utilize Optional Attributes","text":"<p>Use Python's Optional type and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>from typing import Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"concepts/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>class UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in message.</p>"},{"location":"concepts/prompting/#simplification-with-the-maybe-pattern","title":"Simplification with the Maybe Pattern","text":"<p>You can further simplify this using instructor to create the <code>Maybe</code> pattern dynamically from any <code>BaseModel</code>.</p> <pre><code>import instructor\n\nMaybeUser = instructor.Maybe(UserDetail)\n</code></pre> <p>This allows you to quickly create a Maybe type for any class, streamlining the process.</p>"},{"location":"concepts/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from enum import Enum, auto\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(description=\"Correctly assign one of the predefined roles to the user.\")\n</code></pre> <p>If you're having a hard time with <code>Enum</code> and alternative is to use <code>Literal</code></p> <pre><code>class UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre> <p>If you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings.</p>"},{"location":"concepts/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>class Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n    instructions: str = Field(..., description=\"Restate the instructions and rules to correctly determine the title.\")\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>from typing import List\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(..., description=\"Extract any other properties that might be relevant.\")\n</code></pre>"},{"location":"concepts/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>class Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\")\n</code></pre> <p>Using Tuples for Simple Types</p> <p>For simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions.</p> <pre><code>class UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\")\n</code></pre>"},{"location":"concepts/prompting/#advanced-arbitrary-properties","title":"Advanced Arbitrary Properties","text":"<p>For multiple users, aim to use consistent key names when extracting properties.</p> <pre><code>class UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users.\n    Use consistent key names for properties across users.\n    \"\"\"\n    users: List[UserDetail]\n</code></pre> <p>This refined guide should offer a cleaner and more organized approach to structure engineering in Python.</p>"},{"location":"concepts/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field:</p> <pre><code>class UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(..., description=\"Correct and complete list of friend IDs, representing relationships between users.\")\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(..., description=\"Collection of users, correctly capturing the relationships among them.\")\n</code></pre>"},{"location":"concepts/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work_time and leisure_time.</p> <pre><code>class TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(..., description=\"Time range during which the user is working.\")\n    leisure_time: TimeRange = Field(..., description=\"Time range reserved for leisure activities.\")\n</code></pre> <p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>class TimeRange(BaseModel):\n    chain_of_thought: str = Field(..., description=\"Step by step reasoning to get the correct time range\")\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n</code></pre>"},{"location":"concepts/reask_validation/","title":"Validation and Reasking","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the systen can use to self correct.</p>"},{"location":"concepts/reask_validation/#pydantic","title":"Pydantic","text":"<p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Good llm validation is just good validation</p> <p>If you want to see some more examples on validators checkout our blog post Good llm validation is just good validation</p>"},{"location":"concepts/reask_validation/#code-based-validation-example","title":"Code-Based Validation Example","text":"<p>First define a Pydantic model with a validator using the <code>Annotation</code> class from <code>typing_extensions</code>.</p> <p>Enforce a naming rule using Pydantic's built-in validation:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-code-based-validation","title":"Output for Code-Based Validation","text":"<pre><code>1 validation error for UserDetail\nname\n   Value error, name must contain a space (type=value_error)\n</code></pre> <p>As we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model.</p>"},{"location":"concepts/reask_validation/#llm-based-validation-example","title":"LLM-Based Validation Example","text":"<p>LLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(OpenAI())\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\", openai_client=client))\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-llm-based-validation","title":"Output for LLM-Based Validation","text":"<p>Its important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"concepts/reask_validation/#using-reasking-logic-to-correct-outputs","title":"Using Reasking Logic to Correct Outputs","text":"<p>Validators are a great tool for ensuring some property of the outputs. When you use the <code>patch()</code> method with the <code>openai</code> client, you can use the <code>max_retries</code> parameter to set the number of times you can reask the model to correct the output.</p> <p>Its a great layer of defense against bad outputs of two forms.</p> <ol> <li>Pydantic Validation Errors (code or llm based)</li> <li>JSON Decoding Errors (when the model returns a bad response)</li> </ol>"},{"location":"concepts/reask_validation/#step-1-define-the-response-model-with-validators","title":"Step 1: Define the Response Model with Validators","text":"<p>Noticed the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a <code>ValueError</code> if the name is not in uppercase.</p> <pre><code>import instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(OpenAI())\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre>"},{"location":"concepts/reask_validation/#step-2-using-the-client-with-retries","title":"Step 2. Using the Client with Retries","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>model = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n</code></pre>"},{"location":"concepts/reask_validation/#what-happens-behind-the-scenes","title":"What happens behind the scenes?","text":"<p>Behind the scenes, the <code>instructor.patch()</code> method adds a <code>max_retries</code> parameter to the <code>openai.ChatCompletion.create()</code> method. The <code>max_retries</code> parameter will trigger up to 2 reattempts if the <code>name</code> attribute fails the uppercase validation in <code>UserDetails</code>.</p> <pre><code>try:\n    ...\nexcept (ValidationError, JSONDecodeError) as e:\n    kwargs[\"messages\"].append(response.choices[0].message)\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n</code></pre>"},{"location":"concepts/reask_validation/#advanced-validation-techniques","title":"Advanced Validation Techniques","text":"<p>The docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better, for a example of model level validation, and using a validation context check out our example on verifying citations which covers</p> <ol> <li>Validate the entire object with all attributes rather than one attribute at a time</li> <li>Using some 'context' to validate the object, in this case we use the <code>context</code> to check if the citation existed in the original text.</li> </ol>"},{"location":"concepts/reask_validation/#takeaways","title":"Takeaways","text":"<p>By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content but also pave the way for more autonomous and effective systems.</p>"},{"location":"concepts/typeadapter/","title":"Type Adapter","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/types/","title":"Types","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/union/","title":"Union","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"examples/","title":"Function Calls by Example","text":""},{"location":"examples/#quick-links","title":"Quick Links","text":"<ol> <li>How are single and multi-label classifications done using enums?</li> <li>How is AI self-assessment implemented with <code>llm_validator</code>?</li> <li>How are exact citations retrieved using regular expressions and smart prompting?</li> <li>How are search queries segmented through function calling and multi-task definitions?</li> <li>How are knowledge graphs generated from questions?</li> <li>How are complex queries decomposed into subqueries in a single request?</li> <li>How are entities extracted and resolved from documents?</li> <li>How are recursive schemas implemented and understood?</li> <li>How are tables extracted automatically from textual data?</li> <li>How is multi-file code generation accomplished?</li> <li>How is Personally Identifiable Information sanitized from documents?</li> <li>How are action items and dependencies generated from transcripts?</li> <li>How to enable OpenAI's moderation</li> </ol> <p>Explore more!</p>"},{"location":"examples/action_items/","title":"Example: Extracting Action Items from Meeting Transcripts","text":"<p>In this guide, we'll walk through how to extract action items from meeting transcripts using OpenAI's API and Pydantic. This use case is essential for automating project management tasks, such as task assignment and priority setting.</p> <p>Motivation</p> <p>Significant amount of time is dedicated to meetings, where action items are generated as the actionable outcomes of these discussions. Automating the extraction of action items can save time and guarantee that no critical tasks are overlooked.</p>"},{"location":"examples/action_items/#defining-the-structures","title":"Defining the Structures","text":"<p>We'll model a meeting transcript as a collection of <code>Ticket</code> objects, each representing an action item. Every <code>Ticket</code> can have multiple <code>Subtask</code> objects, representing smaller, manageable pieces of the main task.</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass PriorityEnum(str, Enum):\n    high = \"High\"\n    medium = \"Medium\"\n    low = \"Low\"\n\nclass Subtask(BaseModel):\n    \"\"\"Correctly resolved subtask from the given transcript\"\"\"\n    id: int\n    name: str\n\nclass Ticket(BaseModel):\n    \"\"\"Correctly resolved ticket from the given transcript\"\"\"\n    id: int\n    name: str\n    description: str\n    priority: PriorityEnum\n    assignees: List[str]\n    subtasks: Optional[List[Subtask]]\n    dependencies: Optional[List[int]]\n\nclass ActionItems(BaseModel):\n    \"\"\"Correctly resolved set of action items from the given transcript\"\"\"\n    items: List[Ticket]\n</code></pre>"},{"location":"examples/action_items/#extracting-action-items","title":"Extracting Action Items","text":"<p>To extract action items from a meeting transcript, we use the <code>generate</code> function. It calls OpenAI's API, processes the text, and returns a set of action items modeled as <code>ActionItems</code>.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\ndef generate(data: str) -&gt; ActionItems:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=ActionItems,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a transcript of a meeting...\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create the action items for the following transcript: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/action_items/#evaluation-and-testing","title":"Evaluation and Testing","text":"<p>To test the <code>generate</code> function, we provide it with a sample transcript, and then print the JSON representation of the extracted action items.</p> <pre><code>prediction = generate(\n\"\"\"\nAlice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority.\n\nBob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on?\n\nAlice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks.\n\nCarol: I can help with the front-end part of the authentication system.\n\nBob: Great, Carol. I'll handle the back-end optimization then.\n\nAlice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.\n\nCarol: Is the new billing system already in place?\n\nAlice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system?\n\nBob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.\n\nAlice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.\n\nCarol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that.\n\nAlice: Sounds like a plan. Let's get these tasks modeled out and get started.\"\"\"\n)\n</code></pre>"},{"location":"examples/action_items/#visualizing-the-tasks","title":"Visualizing the tasks","text":"<p>In order to quickly visualize the data we used code interpreter to create a graphviz export of the json version of the ActionItems array.</p> <p></p> <pre><code>{\n  \"items\": [\n    {\n      \"id\": 1,\n      \"name\": \"Improve Authentication System\",\n      \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\",\n      \"priority\": \"High\",\n      \"assignees\": [\"Bob\", \"Carol\"],\n      \"subtasks\": [\n        {\n          \"id\": 2,\n          \"name\": \"Front-end Revamp\"\n        },\n        {\n          \"id\": 3,\n          \"name\": \"Back-end Optimization\"\n        }\n      ],\n      \"dependencies\": []\n    },\n    {\n      \"id\": 4,\n      \"name\": \"Integrate Authentication System with Billing System\",\n      \"description\": \"Integrate the improved authentication system with the new billing system\",\n      \"priority\": \"Medium\",\n      \"assignees\": [\"Bob\"],\n      \"subtasks\": [],\n      \"dependencies\": [1]\n    },\n    {\n      \"id\": 5,\n      \"name\": \"Update User Documentation\",\n      \"description\": \"Update the user documentation to reflect the changes in the authentication system\",\n      \"priority\": \"Low\",\n      \"assignees\": [\"Carol\"],\n      \"subtasks\": [],\n      \"dependencies\": [2]\n    }\n  ]\n}\n</code></pre> <p>In this example, the <code>generate</code> function successfully identifies and segments the action items, assigning them priorities, assignees, subtasks, and dependencies as discussed in the meeting.</p> <p>By automating this process, you can ensure that important tasks and details are not lost in the sea of meeting minutes, making project management more efficient and effective.</p>"},{"location":"examples/autodataframe/","title":"Example: Converting Text into Dataframes","text":"<p>In this example, we'll demonstrate how to convert a text into dataframes using OpenAI Function Call. We will define the necessary data structures using Pydantic and show how to convert the text into dataframes.</p> <p>Motivation</p> <p>Often times when we parse data we have an opportunity to extract structured data, what if we could extract an arbitrary number of tables with arbitrary schemas? By pulling out dataframes we could write tables or .csv files and attach them to our retrieved data.</p>"},{"location":"examples/autodataframe/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>Let's start by defining the data structures required for this task: <code>RowData</code>, <code>Dataframe</code>, and <code>Database</code>.</p> <pre><code>from pydantic import Field, BaseModel\nfrom typing import List, Any\n\n\nclass RowData(BaseModel):\n    row: List[Any] = Field(..., description=\"The values for each row\")\n    citation: str = Field(\n        ..., description=\"The citation for this row from the original source data\"\n    )\n\n\nclass Dataframe(BaseModel):\n    \"\"\"\n    Class representing a dataframe. This class is used to convert\n    data into a frame that can be used by pandas.\n    \"\"\"\n\n    name: str = Field(..., description=\"The name of the dataframe\")\n    data: List[RowData] = Field(\n        ...,\n        description=\"Correct rows of data aligned to column names, Nones are allowed\",\n    )\n    columns: List[str] = Field(\n        ...,\n        description=\"Column names relevant from source data, should be in snake_case\",\n    )\n\n    def to_pandas(self):\n        import pandas as pd\n\n        columns = self.columns + [\"citation\"]\n        data = [row.row + [row.citation] for row in self.data]\n\n        return pd.DataFrame(data=data, columns=columns)\n\n\nclass Database(BaseModel):\n    \"\"\"\n    A set of correct named and defined tables as dataframes\n    \"\"\"\n\n    tables: List[Dataframe] = Field(\n        ...,\n        description=\"List of tables in the database\",\n    )\n</code></pre> <p>The <code>RowData</code> class represents a single row of data in the dataframe. It contains a <code>row</code> attribute for the values in each row and a <code>citation</code> attribute for the citation from the original source data.</p> <p>The <code>Dataframe</code> class represents a dataframe and consists of a <code>name</code> attribute, a list of <code>RowData</code> objects in the <code>data</code> attribute, and a list of column names in the <code>columns</code> attribute. It also provides a <code>to_pandas</code> method to convert the dataframe into a Pandas DataFrame.</p> <p>The <code>Database</code> class represents a set of tables in a database. It contains a list of <code>Dataframe</code> objects in the <code>tables</code> attribute.</p>"},{"location":"examples/autodataframe/#using-the-prompt-pipeline","title":"Using the Prompt Pipeline","text":"<p>To convert a text into dataframes, we'll use the Prompt Pipeline in OpenAI Function Call. We can define a function <code>dataframe</code> that takes a text as input and returns a <code>Database</code> object.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef dataframe(data: str) -&gt; Database:\n    return client.chat.completions.create(\n        model=\"gpt-4-0613\",\n        temperature=0.1,\n        response_model=Database,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"Map this data into a dataframe a\n                nd correctly define the correct columns and rows\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"{data}\",\n            },\n        ],\n        max_tokens=1000,\n    )\n</code></pre> <p>The <code>dataframe</code> function takes a string <code>data</code> as input and creates a completion using the Prompt Pipeline. It prompts the model to map the data into a dataframe and define the correct columns and rows. The resulting completion is then converted into a <code>Database</code> object.</p>"},{"location":"examples/autodataframe/#evaluating-an-example","title":"Evaluating an Example","text":"<p>Let's evaluate the example by converting a text into dataframes using the <code>dataframe</code> function and print the resulting dataframes.</p> <pre><code>dfs = dataframe(\"\"\"My name is John and I am 25 years old. I live in\nNew York and I like to play basketball. His name is\nMike and he is 30 years old. He lives in San Francisco\nand he likes to play baseball. Sarah is 20 years old\nand she lives in Los Angeles. She likes to play tennis.\nHer name is Mary and she is 35 years old.\nShe lives in Chicago.\n\nOn one team 'Tigers' the captain is John and there are 12 players.\nOn the other team 'Lions' the captain is Mike and there are 10 players.\n\"\"\")\n\nfor df in dfs.tables:\n    print(df.name)\n    print(df.to_pandas())\n</code></pre> <p>The output will be:</p> <pre><code>People\nName  Age           City Favorite Sport\n0   John   25       New York     Basketball\n1   Mike   30  San Francisco       Baseball\n2  Sarah   20    Los Angeles         Tennis\n3   Mary   35        Chicago           None\n\nTeams\nTeam Name Captain  Number of Players\n0    Tigers    John                 12\n1     Lions    Mike                 10\n</code></pre>"},{"location":"examples/classification/","title":"Example: Text Classification using OpenAI and Pydantic","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using the OpenAI API, Python's <code>enum</code> module, and Pydantic models.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using OpenAI's GPT models in combination with Python data structures.</p>"},{"location":"examples/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a Pydantic model for the output.</p> <pre><code>import enum\nfrom pydantic import BaseModel\n\nclass Labels(str, enum.Enum):\n    \"\"\"Enumeration for single-label text classification.\"\"\"\n    SPAM = \"spam\"\n    NOT_SPAM = \"not_spam\"\n\nclass SinglePrediction(BaseModel):\n    \"\"\"\n    Class for a single class label prediction.\n    \"\"\"\n    class_label: Labels\n</code></pre>"},{"location":"examples/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\ndef classify(data: str) -&gt; SinglePrediction:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=SinglePrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code># Test single-label classification\nprediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\")\nassert prediction.class_label == Labels.SPAM\n</code></pre>"},{"location":"examples/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <pre><code># Define Enum class for multiple labels\nclass MultiLabels(str, enum.Enum):\n    TECH_ISSUE = \"tech_issue\"\n    BILLING = \"billing\"\n    GENERAL_QUERY = \"general_query\"\n\n# Define the multi-class prediction model\nclass MultiClassPrediction(BaseModel):\n    \"\"\"\n    Class for a multi-class label prediction.\n    \"\"\"\n    class_labels: List[MultiLabels]\n</code></pre>"},{"location":"examples/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> is responsible for multi-label classification.</p> <pre><code>def multi_classify(data: str) -&gt; MultiClassPrediction:\n    \"\"\"Perform multi-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code># Test multi-label classification\nticket = \"My account is locked and I can't access my billing info.\"\nprediction = multi_classify(ticket)\nassert MultiLabels.TECH_ISSUE in prediction.class_labels\nassert MultiLabels.BILLING in prediction.class_labels\n</code></pre>"},{"location":"examples/entity_resolution/","title":"Entity Resolution and Visualization for Legal Documents","text":"<p>In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents.</p> <p>Motivation</p> <p>Legal contracts are full of intricate details and interconnected clauses. Automatically extracting and visualizing these elements can make it easier to understand the document's overall structure and terms.</p>"},{"location":"examples/entity_resolution/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>The <code>Entity</code> and <code>Property</code> classes model extracted entities and their attributes. <code>DocumentExtraction</code> encapsulates a list of these entities.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n    resolved_absolute_value: str\n\n\nclass Entity(BaseModel):\n    id: int = Field(\n        ...,\n        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n    )\n    subquote_string: List[str] = Field(\n        ...,\n        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n    )\n    entity_title: str\n    properties: List[Property] = Field(\n        ..., description=\"List of properties of the entity\"\n    )\n    dependencies: List[int] = Field(\n        ...,\n        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n    )\n\n\nclass DocumentExtraction(BaseModel):\n    entities: List[Entity] = Field(\n        ...,\n        description=\"Body of the answer, each fact should be its seperate object with a body and a list of sources\",\n    )\n</code></pre>"},{"location":"examples/entity_resolution/#entity-extraction-and-resolution","title":"Entity Extraction and Resolution","text":"<p>The <code>ask_ai</code> function utilizes OpenAI's API to extract and resolve entities from the input content.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\ndef ask_ai(content) -&gt; DocumentExtraction:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=DocumentExtraction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract and resolve a list of entities from the following document:\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": content,\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/entity_resolution/#graph-visualization","title":"Graph Visualization","text":"<p><code>generate_graph</code> takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies.</p> <pre><code>from graphviz import Digraph\n\ndef generate_html_label(entity: Entity) -&gt; str:\n    rows = [f\"&lt;tr&gt;&lt;td&gt;{prop.key}&lt;/td&gt;&lt;td&gt;{prop.resolved_absolute_value}&lt;/td&gt;&lt;/tr&gt;\" for prop in entity.properties]\n    table_rows = \"\".join(rows)\n    return f\"&lt;table border='0' cellborder='1' cellspacing='0'&gt;&lt;tr&gt;&lt;td colspan='2'&gt;&lt;b&gt;{entity.entity_title}&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;{table_rows}&lt;/table&gt;&gt;\"\n\ndef generate_graph(data: DocumentExtraction):\n    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n\n    for entity in data.entities:\n        label = generate_html_label(entity)\n        dot.node(str(entity.id), label)\n\n    for entity in data.entities:\n        for dep_id in entity.dependencies:\n            dot.edge(str(entity.id), str(dep_id))\n\n    dot.render(\"entity.gv\", view=True)\n</code></pre>"},{"location":"examples/entity_resolution/#execution","title":"Execution","text":"<p>Finally, execute the code to visualize the entity graph for the sample legal contract.</p> <pre><code>content = \"\"\"\nSample Legal Contract\nAgreement Contract\n\nThis Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n\nArticle 1: Scope of Work\n\nThe Service Provider will deliver the software product to the Client 30 days after the agreement date.\n\nArticle 2: Payment Terms\n\nThe total payment for the service is $50,000.\nAn initial payment of $10,000 will be made within 7 days of the the signed date.\nThe final payment will be due 45 days after [SignDate].\n\nArticle 3: Confidentiality\n\nThe parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n\nArticle 4: Termination\n\nThe contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n\"\"\"  # Your legal contract here\nmodel = ask_ai(content)\ngenerate_graph(model)\n</code></pre> <p>This will produce a graphical representation of the entities and their dependencies, stored as \"entity.gv\".</p> <p></p>"},{"location":"examples/exact_citations/","title":"Example: Answering Questions with Validated Citations","text":"<p>For the full code example check out examples/citation_fuzzy_match.py</p>"},{"location":"examples/exact_citations/#overview","title":"Overview","text":"<p>This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, <code>Fact</code> and <code>QuestionAnswer</code>, are defined to encapsulate the information of individual facts and the entire answer, respectively.</p>"},{"location":"examples/exact_citations/#data-structures","title":"Data Structures","text":""},{"location":"examples/exact_citations/#the-fact-class","title":"The <code>Fact</code> Class","text":"<p>The <code>Fact</code> class encapsulates a single statement or fact. It contains two fields:</p> <ul> <li><code>fact</code>: A string representing the body of the fact or statement.</li> <li><code>substring_quote</code>: A list of strings. Each string is a direct quote from the context that supports the <code>fact</code>.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method validates the sources (<code>substring_quote</code>) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list.</p> <pre><code>from pydantic import Field, BaseModel, model_validator, FieldValidationInfo\nfrom typing import List\n\nclass Fact(BaseModel):\n    fact: str = Field(...)\n    substring_quote: List[str] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: FieldValidationInfo) -&gt; \"Fact\":\n        text_chunks = info.context.get(\"text_chunk\", None)\n        spans = list(self.get_spans(text_chunks))\n        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def get_spans(self, context):\n        for quote in self.substring_quote:\n            yield from self._get_span(quote, context)\n\n    def _get_span(self, quote, context):\n        for match in re.finditer(re.escape(quote), context):\n            yield match.span()\n</code></pre>"},{"location":"examples/exact_citations/#the-questionanswer-class","title":"The <code>QuestionAnswer</code> Class","text":"<p>This class encapsulates the question and its corresponding answer. It contains two fields:</p> <ul> <li><code>question</code>: The question asked.</li> <li><code>answer</code>: A list of <code>Fact</code> objects that make up the answer.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources_1","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method checks that each <code>Fact</code> object in the <code>answer</code> list has at least one valid source. If a <code>Fact</code> object has no valid sources, it is removed from the <code>answer</code> list.</p> <pre><code>class QuestionAnswer(BaseModel):\n    question: str = Field(...)\n    answer: List[Fact] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self) -&gt; \"QuestionAnswer\":\n        self.answer = [fact for fact in self.answer if len(fact.substring_quote) &gt; 0]\n        return self\n</code></pre>"},{"location":"examples/exact_citations/#function-to-ask-ai-a-question","title":"Function to Ask AI a Question","text":""},{"location":"examples/exact_citations/#the-ask_ai-function","title":"The <code>ask_ai</code> Function","text":"<p>This function takes a string <code>question</code> and a string <code>context</code> and returns a <code>QuestionAnswer</code> object. It uses the OpenAI API to fetch the answer and then validates the sources using the defined classes.</p> <p>To understand the validation context work from pydantic check out pydantic's docs</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model, validation_context keyword\nclient = instructor.patch(OpenAI())\n\ndef ask_ai(question: str, context: str) -&gt; QuestionAnswer:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0,\n        response_model=QuestionAnswer,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\"},\n            {\"role\": \"user\", \"content\": f\"{context}\"},\n            {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n        ],\n        validation_context={\"text_chunk\": context},\n    )\n</code></pre>"},{"location":"examples/exact_citations/#example","title":"Example","text":"<p>dd Here's an example of using these classes and functions to ask a question and validate the answer.</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\n</code></pre> <p>The output would be a <code>QuestionAnswer</code> object containing validated facts and their sources.</p> <pre><code>{\n  \"question\": \"where did he go to school?\",\n  \"answer\": [\n    {\n      \"statement\": \"Jason Liu went to an arts highschool.\",\n      \"substring_phrase\": [\n        \"arts highschool\"\n      ]\n    },\n    {\n      \"statement\": \"Jason Liu studied Computational Mathematics and physics in university.\",\n      \"substring_phrase\": [\n        \"university\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>This ensures that every piece of information in the answer has been validated against the context.</p>"},{"location":"examples/gpt-engineer/","title":"Example: Creating Multiple Files Program","text":"<p>This example shows how to create a multiple files program based on specifications by utilizing the OpenAI Function Call. We will define the necessary data structures using Pydantic and demonstrate how to convert a specification (prompt) into multiple files.</p> <p>Motivation</p> <p>Creating multiple file programs based on specifications is a challenging and rewarding skill that can help you build complex and scalable applications. With OpenAI Function Call, you can leverage the power of language models to generate an entire codebase and code snippets that match your specifications.</p>"},{"location":"examples/gpt-engineer/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>Let's start by defining the data structure of <code>File</code> and <code>Program</code>.</p> <pre><code>from typing import List\nfrom pydantic import Field\nfrom instructor import BaseModel\n\n\nclass File(BaseModel):\n    \"\"\"\n    Correctly named file with contents.\n    \"\"\"\n\n    file_name: str = Field(\n        ..., description=\"The name of the file including the extension\"\n    )\n    body: str = Field(..., description=\"Correct contents of a file\")\n\n    def save(self):\n        with open(self.file_name, \"w\") as f:\n            f.write(self.body)\n\n\nclass Program(BaseModel):\n    \"\"\"\n    Set of files that represent a complete and correct program\n    \"\"\"\n\n    files: List[File] = Field(..., description=\"List of files\")\n</code></pre> <p>The <code>File</code> class represents a single file or script, and it contains a <code>name</code> attribute and <code>body</code> for the text content of the file. Notice that we added the <code>save</code> method to the <code>File</code> class. This method is used to writes the body of the file to disk using the name as path.</p> <p>The <code>Program</code> class represents a collection of files that form a complete and correct program. It contains a list of <code>File</code> objects in the <code>files</code> attribute.</p>"},{"location":"examples/gpt-engineer/#calling-completions","title":"Calling Completions","text":"<p>To create the files, we will use the base <code>openai</code> API. We can define a function that takes in a string and returns a <code>Program</code> object.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef develop(data: str) -&gt; Program:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0.1,\n        response_model=Program,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include __init__.py files and write correct python code with correct imports.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": data,\n            },\n        ],\n        max_tokens=1000,\n    )\n</code></pre>"},{"location":"examples/gpt-engineer/#evaluating-an-example","title":"Evaluating an Example","text":"<p>Let's evaluate the example by specifying the program to create and print the resulting files.</p> <pre><code>program = develop(\n        \"\"\"\n        Create a fastapi app with a readme.md file and a main.py file with\n        some basic math functions. the datamodels should use pydantic and\n        the main.py should use fastapi. the readme.md should have a title\n        and a description. The readme should contain some helpful infromation\n        and a curl example\"\"\"\n    )\n\nfor file in program.files:\n    print(file.file_name)\n    print(\"-\")\n    print(file.body)\n    print(\"\\n\\n\\n\")\n</code></pre> <p>The output will be:</p> <pre><code># readme.md\n\n- # FastAPI App\n\n  This is a FastAPI app that provides some basic math functions.\n\n  ## Usage\n\n  To use this app, follow the instructions below:\n\n  1. Install the required dependencies by running `pip install -r requirements.txt`.\n  2. Start the app by running `uvicorn main:app --reload`.\n  3. Open your browser and navigate to `http://localhost:8000/docs` to access the Swagger UI documentation.\n\n  ## Example\n\n  You can use the following curl command to test the `/add` endpoint:\n\n  ```bash\n  $ curl -X POST -H \"Content-Type: application/json\" -d '{\"a\": 2, \"b\": 3}' http://localhost:8000/add\n  ```\n</code></pre> <pre><code># main.py\n-\n    from fastapi import FastAPI\n    from pydantic import BaseModel\n\n    app = FastAPI()\n\n\n    class Numbers(BaseModel):\n        a: int\n        b: int\n\n\n    @app.post('/add')\n    def add_numbers(numbers: Numbers):\n        return {'result': numbers.a + numbers.b}\n\n\n    @app.post('/subtract')\n    def subtract_numbers(numbers: Numbers):\n        return {'result': numbers.a - numbers.b}\n\n\n    @app.post('/multiply')\n    def multiply_numbers(numbers: Numbers):\n        return {'result': numbers.a * numbers.b}\n\n\n    @app.post('/divide')\n    def divide_numbers(numbers: Numbers):\n        if numbers.b == 0:\n            return {'error': 'Cannot divide by zero'}\n        return {'result': numbers.a / numbers.b}\n</code></pre> <pre><code># requirements.txt\n\n- fastapi\n  uvicorn\n  pydantic\n</code></pre>"},{"location":"examples/gpt-engineer/#add-refactoring-capabilities","title":"Add Refactoring Capabilities","text":"<p>This second part of the example shows how OpenAI API can be used to update the multiples files previously created, based on new specifications.</p> <p>In order to do that, we'll rely on the standard unidiff format.</p> <p>This will be our definition for a change in our code base:</p> <pre><code>from pydantic import Field\nfrom instructor import BaseModel\n\nclass Diff(BaseModel):\n    \"\"\"\n    Changes that must be correctly made in a program's code repository defined as a\n    complete diff (Unified Format) file which will be used to `patch` the repository.\n\n    Example:\n      --- /path/to/original timestamp\n      +++ /path/to/new  timestamp\n      @@ -1,3 +1,9 @@\n      +This is an important\n      +notice! It should\n      +therefore be located at\n      +the beginning of this\n      +document!\n      +\n       This part of the\n       document has stayed the\n       same from version to\n      @@ -8,13 +14,8 @@\n       compress the size of the\n       changes.\n      -This paragraph contains\n      -text that is outdated.\n      -It will be deleted in the\n      -near future.\n      -\n       It is important to spell\n      -check this dokument. On\n      +check this document. On\n       the other hand, a\n       misspelled word isn't\n       the end of the world.\n      @@ -22,3 +23,7 @@\n       this paragraph needs to\n       be changed. Things can\n       be added after it.\n      +\n      +This paragraph contains\n      +important new additions\n      +to this document.\n    \"\"\"\n\n    diff: str = Field(\n        ...,\n        description=(\n            \"Changes in a code repository correctly represented in 'diff' format, \"\n            \"correctly escaped so it could be used in a JSON\"\n        ),\n    )\n</code></pre> <p>The <code>diff</code> class represents a diff file, with a set of changes that can be applied to our program using a tool like patch or Git.</p>"},{"location":"examples/gpt-engineer/#calling-refactor-completions","title":"Calling Refactor Completions","text":"<p>We'll define a function that will pass the program and the new specifications to the OpenAI API:</p> <pre><code>from generate import Program\n\ndef refactor(new_requirements: str, program: Program) -&gt; Diff:\n    program_description = \"\\n\".join(\n        [f\"{code.file_name}\\n[[[\\n{code.body}\\n]]]\\n\" for code in program.files]\n    )\n    return client.chat.completions.create(\n        # model=\"gpt-3.5-turbo-0613\",\n        model=\"gpt-4\",\n        temperature=0,\n        response_model=Diff,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class programming AI capable of refactor \"\n                \"existing python repositories. You will name files correct, include \"\n                \"__init__.py files and write correct python code, with correct imports. \"\n                \"You'll deliver your changes in valid 'diff' format so that they could \"\n                \"be applied using the 'patch' command. \"\n                \"Make sure you put the correct line numbers, \"\n                \"and that all lines that must be changed are correctly marked.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": new_requirements,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": program_description,\n            },\n        ],\n        max_tokens=1000,\n    )\n</code></pre> <p>Notice we're using here the version <code>gpt-4</code> of the model, which is more powerful but, also, more expensive.</p>"},{"location":"examples/gpt-engineer/#creating-an-example-refactoring","title":"Creating an Example Refactoring","text":"<p>To tests these refactoring, we'll use the <code>program</code> object, generated in the first part of this example.</p> <pre><code>changes = refactor(\n    new_requirements=\"Refactor this code to use flask instead.\",\n    program=program,\n)\nprint(changes.diff)\n</code></pre> <p>The output will be this:</p> <pre><code>--- readme.md\n+++ readme.md\n@@ -1,9 +1,9 @@\n # FastAPI App\n\n-This is a FastAPI app that provides some basic math functions.\n+This is a Flask app that provides some basic math functions.\n\n ## Usage\n\n To use this app, follow the instructions below:\n\n 1. Install the required dependencies by running `pip install -r requirements.txt`.\n-2. Start the app by running `uvicorn main:app --reload`.\n+2. Start the app by running `flask run`.\n 3. Open your browser and navigate to `http://localhost:5000/docs` to access the Swagger UI documentation.\n\n ## Example\n\n To perform a basic math operation, you can use the following curl command:\n\n ```bash\n-curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": [2, 3]}' http://localhost:8000/calculate\n+curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": [2, 3]}' http://localhost:5000/calculate\n</code></pre> <p>--- main.py +++ main.py @@ -1,29 +1,29 @@ -from fastapi import FastAPI -from pydantic import BaseModel +from flask import Flask, request, jsonify</p> <p>-app = FastAPI() +app = Flask(name)</p> <p>-class Operation(BaseModel):</p> <ul> <li>operation: str</li> <li> <p>operands: list   +@app.route('/calculate', methods=['POST'])   +def calculate():</p> </li> <li> <p>data = request.get_json()</p> </li> <li>operation = data.get('operation')</li> <li>operands = data.get('operands')</li> </ul> <p>-@app.post('/calculate') -async def calculate(operation: Operation):</p> <ul> <li>if operation.operation == 'add':</li> <li>result = sum(operation.operands)</li> <li>elif operation.operation == 'subtract':</li> <li>result = operation.operands[0] - sum(operation.operands[1:])</li> <li> <p>elif operation.operation == 'multiply':</p> </li> <li> <p>if operation == 'add':</p> </li> <li>result = sum(operands)</li> <li>elif operation == 'subtract':</li> <li>result = operands[0] - sum(operands[1:])</li> <li> <p>elif operation == 'multiply':   result = 1</p> </li> <li> <p>for operand in operation.operands:</p> </li> <li> <p>for operand in operands:              result *= operand</p> </li> <li> <p>elif operation.operation == 'divide':</p> </li> <li>result = operation.operands[0]</li> <li> <p>for operand in operation.operands[1:]:</p> </li> <li> <p>elif operation == 'divide':</p> </li> <li>result = operands[0]</li> <li> <p>for operand in operands[1:]:              result /= operand   else:   result = None</p> </li> <li> <p>return {'result': result}</p> </li> <li> <p>return jsonify({'result': result})</p> </li> </ul> <p>--- requirements.txt +++ requirements.txt @@ -1,3 +1,2 @@ -fastapi -uvicorn -pydantic +flask +flask-cors</p> <pre><code>\n</code></pre>"},{"location":"examples/knowledge_graph/","title":"Visualizing Knowledge Graphs for Complex Topics","text":"<p>In this guide, you'll discover how to visualize a detailed knowledge graph for understanding complex topics, in this case, quantum mechanics. We leverage OpenAI's API and the Graphviz library to bring structure to intricate subjects.</p> <p>Motivation</p> <p>Knowledge graphs offer a visually appealing and coherent way to understand complicated topics like quantum mechanics. By generating these graphs automatically, you can accelerate the learning process and make it easier to digest complex information.</p>"},{"location":"examples/knowledge_graph/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's model a knowledge graph with <code>Node</code> and <code>Edge</code> objects. <code>Node</code> objects represent key concepts or entities, while <code>Edge</code> objects indicate the relationships between them.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n</code></pre>"},{"location":"examples/knowledge_graph/#generating-knowledge-graphs","title":"Generating Knowledge Graphs","text":"<p>The <code>generate_graph</code> function leverages OpenAI's API to generate a knowledge graph based on the input query.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Adds response_model to ChatCompletion\n# Allows the return of Pydantic model rather than raw JSON\nclient = instructor.patch(OpenAI())\n\ndef generate_graph(input) -&gt; KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )  # type: ignore\n</code></pre>"},{"location":"examples/knowledge_graph/#visualizing-the-graph","title":"Visualizing the Graph","text":"<p>The <code>visualize_knowledge_graph</code> function uses the Graphviz library to render the generated knowledge graph.</p> <pre><code>from graphviz import Digraph\n\ndef visualize_knowledge_graph(kg: KnowledgeGraph):\n    dot = Digraph(comment=\"Knowledge Graph\")\n\n    # Add nodes\n    for node in kg.nodes:\n        dot.node(str(node.id), node.label, color=node.color)\n\n    # Add edges\n    for edge in kg.edges:\n        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n    # Render the graph\n    dot.render(\"knowledge_graph.gv\", view=True)\n</code></pre>"},{"location":"examples/knowledge_graph/#putting-it-all-together","title":"Putting It All Together","text":"<p>Execute the code to generate and visualize a knowledge graph for understanding quantum mechanics.</p> <pre><code>graph: KnowledgeGraph = generate_graph(\"Teach me about quantum mechanics\")\nvisualize_knowledge_graph(graph)\n</code></pre> <p></p> <p>This will produce a visual representation of the knowledge graph, stored as \"knowledge_graph.gv\". You can open this file to explore the key concepts and their relationships in quantum mechanics.</p> <p>By leveraging automated knowledge graphs, you can dissect complex topics into digestible pieces, making the learning journey less daunting and more effective.</p>"},{"location":"examples/moderation/","title":"OpenAI Moderation","text":""},{"location":"examples/moderation/#overview","title":"Overview","text":"<p>This example uses OpenAI's moderation endpoint to check content compliance with OpenAI's usage policies. It can identify and filter harmful content that violates the policies.</p> <p>The model flags content and classifies it into categories including hate, harassment, self-harm, sexual content, and violence. Each category has subcategories for detailed classification.</p> <p>This validator is to be used for monitoring OpenAI API inputs and outputs, other use cases are currently not allowed.</p>"},{"location":"examples/moderation/#incorporating-openai-moderation-validator","title":"Incorporating OpenAI moderation validator","text":"<p>The following code defines a function to validate content using OpenAI's Moderation endpoint. The <code>AfterValidator</code> is used to apply OpenAI's moderation after the compute. This moderation checks if the content complies with OpenAI's usage policies and flags any harmful content. Here's how it works:</p> <ol> <li> <p>Generate the OpenAI client and patch it with the <code>instructor</code>. Patching is not strictly necessary for this example but its a good idea to always patch the client to leverage the full <code>instructor</code> functionality.</p> </li> <li> <p>Annotate our <code>message</code> field with <code>AfterValidator(openai_moderation(client=client))</code>. This means that after the <code>message</code> is computed, it will be passed to the <code>openai_moderation</code> function for validation.</p> </li> </ol> <pre><code>import instructor\n\nfrom instructor import openai_moderation\n\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, AfterValidator, ValidationError\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n</code></pre>"},{"location":"examples/moderation/#testing-openai-moderation-validator","title":"Testing OpenAI moderation validator","text":"<p>Now, let's test our class with a piece of content that violates OpenAI's usage policies.</p> <pre><code>Response(message=\"I want to make them suffer the consequences\")\n</code></pre> <p>The validator will raise a <code>ValidationError</code> if the content violates the policies, like so:</p> <pre><code>ValidationError: 1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n</code></pre> <p>Let's try another example which violates a different policy: self-harm.</p> <pre><code>Response(message=\"I want to hurt myself.\")\n</code></pre> <p>In this case, our validator will flag the output but return a different error message in the trace, clarifying the specific policies that were violated:</p> <p>ValidationError: 1 validation error for Response message Value error, <code>I want to hurt myself</code> was flagged for self_harm, self_harm_intent, violence, self-harm, self-harm/intent [type=value_error, input_value='I want to hurt myself', input_type=str]</p> <pre><code>\n</code></pre>"},{"location":"examples/pii/","title":"PII Data Extraction and Scrubbing","text":""},{"location":"examples/pii/#overview","title":"Overview","text":"<p>This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation.</p>"},{"location":"examples/pii/#defining-the-structures","title":"Defining the Structures","text":"<p>First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n# Define Schemas for PII data\nclass Data(BaseModel):\n    index: int\n    data_type: str\n    pii_value: str\n\nclass PIIDataExtraction(BaseModel):\n    \"\"\"\n    Extracted PII data from a document, all data_types should try to have consistent property names\n    \"\"\"\n    private_data: List[Data]\n\n    def scrub_data(self, content: str) -&gt; str:\n        \"\"\"\n        Iterates over the private data and replaces the value with a placeholder in the form of\n        &lt;{data_type}_{i}&gt;\n        \"\"\"\n        for i, data in enumerate(self.private_data):\n            content = content.replace(data.pii_value, f\"&lt;{data.data_type}_{i}&gt;\")\n        return content\n</code></pre>"},{"location":"examples/pii/#extracting-pii-data","title":"Extracting PII Data","text":"<p>The OpenAI API is utilized to extract PII information from a given document.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\nEXAMPLE_DOCUMENT = \"\"\"\n# Fake Document with PII for Testing PII Scrubbing Model\n# (The content here)\n\"\"\"\n\npii_data: PIIDataExtraction = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=PIIDataExtraction,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EXAMPLE_DOCUMENT,\n        },\n    ]\n)  # type: ignore\n\nprint(\"Extracted PII Data:\")\nprint(pii_data.json(indent=2))\n</code></pre>"},{"location":"examples/pii/#output-of-extracted-pii-data","title":"Output of Extracted PII Data","text":"<pre><code>{\n  \"private_data\": [\n    {\n      \"index\": 0,\n      \"data_type\": \"date\",\n      \"pii_value\": \"01/02/1980\"\n    },\n    {\n      \"index\": 1,\n      \"data_type\": \"ssn\",\n      \"pii_value\": \"123-45-6789\"\n    },\n    {\n      \"index\": 2,\n      \"data_type\": \"email\",\n      \"pii_value\": \"john.doe@email.com\"\n    },\n    {\n      \"index\": 3,\n      \"data_type\": \"phone\",\n      \"pii_value\": \"555-123-4567\"\n    },\n    {\n      \"index\": 4,\n      \"data_type\": \"address\",\n      \"pii_value\": \"123 Main St, Springfield, IL, 62704\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/pii/#scrubbing-pii-data","title":"Scrubbing PII Data","text":"<p>After extracting the PII data, the <code>scrub_data</code> method is used to sanitize the document.</p> <pre><code>print(\"Scrubbed Document:\")\nprint(pii_data.scrub_data(EXAMPLE_DOCUMENT))\n</code></pre>"},{"location":"examples/pii/#output-of-scrubbed-document","title":"Output of Scrubbed Document","text":"<pre><code># Fake Document with PII for Testing PII Scrubbing Model\n\n## Personal Story\n\nJohn Doe was born on &lt;date_0&gt;. His social security number is &lt;ssn_1&gt;. He has been using the email address &lt;email_2&gt; for years, and he can always be reached at &lt;phone_3&gt;.\n\n## Residence\n\nJohn currently resides at &lt;address_4&gt;. He's been living there for about 5 years now.\n</code></pre>"},{"location":"examples/planning-tasks/","title":"Example: Planning and Executing a Query Plan","text":"<p>This example demonstrates how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan in a question-answering system. By breaking down a complex question into smaller sub-questions with defined dependencies, the system can systematically gather the necessary information to answer the main question.</p> <p>Motivation</p> <p>The goal of this example is to showcase how query planning can be used to handle complex questions, facilitate iterative information gathering, automate workflows, and optimize processes. By leveraging the OpenAI Function Call model, you can design and execute a structured plan to find answers effectively.</p> <p>Use Cases:</p> <ul> <li>Complex question answering</li> <li>Iterative information gathering</li> <li>Workflow automation</li> <li>Process optimization</li> </ul> <p>With the OpenAI Function Call model, you can customize the planning process and integrate it into your specific application to meet your unique requirements.</p>"},{"location":"examples/planning-tasks/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's define the necessary Pydantic models to represent the query plan and the queries.</p> <pre><code>import enum\nfrom typing import List\n\nfrom pydantic import Field\n\n\nclass QueryType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n\n    SINGLE_QUESTION = \"SINGLE\"\n    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n\n\nclass Query(BaseModel):\n    \"\"\"Class representing a single question in a query plan.\"\"\"\n\n    id: int = Field(..., description=\"Unique id of the query\")\n    question: str = Field(\n        ...,\n        description=\"Question asked using a question answering system\",\n    )\n    dependancies: List[int] = Field(\n        default_factory=list,\n        description=\"List of sub questions that need to be answered before asking this question\",\n    )\n    node_type: QueryType = Field(\n        default=QueryType.SINGLE_QUESTION,\n        description=\"Type of question, either a single question or a multi-question merge\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n\n    query_graph: List[Query] = Field(\n        ..., description=\"The query graph representing the plan\"\n    )\n\n    def _dependencies(self, ids: List[int]) -&gt; List[Query]:\n        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n        return [q for q in self.query_graph if q.id in ids]\n</code></pre> <p>Graph Generation</p> <p>Notice that this example produces a flat list of items with dependencies that resemble a graph, while pydantic allows for recursive definitions, it's much easier and less confusing for the model to generate flat schemas rather than recursive schemas. If you want to see a recursive example, see recursive schemas</p>"},{"location":"examples/planning-tasks/#planning-a-query-plan","title":"Planning a Query Plan","text":"<p>Now, let's demonstrate how to plan and execute a query plan using the defined models and the OpenAI API.</p> <pre><code>import asyncio\nimport instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\ndef query_planner(question: str) -&gt; QueryPlan:\n    PLANNING_MODEL = \"gpt-4-0613\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n        },\n    ]\n\n    QueryPlan = client.chat.completions.create(\n        model=PLANNING_MODEL,\n        temperature=0,\n        response_model=QueryPlan,\n        messages=messages,\n        max_tokens=1000,\n    )\n    return root\n</code></pre> <pre><code>plan = query_planner(\n    \"What is the difference in populations of Canada and the Jason's home country?\"\n)\nplan.dict()\n</code></pre> <p>No RAG</p> <p>While we build the query plan in this example, we do not propose a method to actually answer the question. You can implement your own answer function that perhaps makes a retrival and calls openai for retrival augmented generation. That step would also make use of function calls but goes beyond the scope of this example.</p> <pre><code>{'query_graph': [{'dependancies': [],\n                    'id': 1,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': \"Identify Jason's home country\"},\n                    {'dependancies': [],\n                    'id': 2,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': 'Find the population of Canada'},\n                    {'dependancies': [1],\n                    'id': 3,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': \"Find the population of Jason's home country\"},\n                    {'dependancies': [2, 3],\n                    'id': 4,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': 'Calculate the difference in populations between Canada and Jason's home country\"}]}\n</code></pre> <p>In the above code, we define a <code>query_planner</code> function that takes a question as input and generates a query plan using the OpenAI API.</p>"},{"location":"examples/planning-tasks/#conclusion","title":"Conclusion","text":"<p>In this example, we demonstrated how to use the OpenAI Function Call <code>ChatCompletion</code> model to plan and execute a query plan using a question-answering system. We defined the necessary structures using Pydantic, created a query planner function.</p> <p>If you want to see multiple versions of this style of code, please visit:</p> <ol> <li>query planning example</li> <li>task planning with topo sort</li> </ol> <p>Feel free to modify the code to fit your specific use case and explore other possibilities of using the OpenAI Function Call model to plan and execute complex workflows.</p>"},{"location":"examples/recursive/","title":"Example: Parsing a Directory Tree","text":"<p>In this example, we will demonstrate how define and use a recursive class definition to convert a string representing a directory tree into a filesystem structure using OpenAI's function call api. We will define the necessary structures using Pydantic, create a function to parse the tree, and provide an example of how to use it.</p>"},{"location":"examples/recursive/#defining-the-structures","title":"Defining the Structures","text":"<p>We will use Pydantic to define the necessary data structures representing the directory tree and its nodes. We have two classes, <code>Node</code> and <code>DirectoryTree</code>, which are used to model individual nodes and the entire directory tree, respectively.</p> <p>Flat is better than nested</p> <p>While it's easier to model things as nested, returning flat items with dependencies tends to yield better results. For a flat example, check out planning tasks where we model a query plan as a dag.</p> <pre><code>import enum\nfrom typing import List\nfrom pydantic import Field\n\nclass NodeType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of nodes in a filesystem.\"\"\"\n    FILE = \"file\"\n    FOLDER = \"folder\"\n\nclass Node(BaseModel):\n    \"\"\"\n    Class representing a single node in a filesystem. Can be either a file or a folder.\n    Note that a file cannot have children, but a folder can.\n\n    Args:\n        name (str): The name of the node.\n        children (List[Node]): The list of child nodes (if any).\n        node_type (NodeType): The type of the node, either a file or a folder.\n\n    Methods:\n        print_paths: Prints the path of the node and its children.\n    \"\"\"\n    name: str = Field(..., description=\"Name of the folder\")\n    children: List[\"Node\"] = Field(\n        default_factory=list,\n        description=\"List of children nodes, only applicable for folders, files cannot have children\",\n    )\n    node_type: NodeType = Field(\n        default=NodeType.FILE,\n        description=\"Either a file or folder, use the name to determine which it could be\",\n    )\n\n    def print_paths(self, parent_path=\"\"):\n        \"\"\"Prints the path of the node and its children.\"\"\"\n        if self.node_type == NodeType.FOLDER:\n            path = f\"{parent_path}/{self.name}\" if parent_path != \"\" else self.name\n            print(path, self.node_type)\n            if self.children is not None:\n                for child in self.children:\n                    child.print_paths(path)\n        else:\n            print(f\"{parent_path}/{self.name}\", self.node_type)\n\nclass DirectoryTree(BaseModel):\n    \"\"\"\n    Container class representing a directory tree.\n\n    Args:\n        root (Node): The root node of the tree.\n\n    Methods:\n        print_paths: Prints the paths of the root node and its children.\n    \"\"\"\n    root: Node = Field(..., description=\"Root folder of the directory tree\")\n\n    def print_paths(self):\n        \"\"\"Prints the paths of the root node and its children.\"\"\"\n        self.root.print_paths()\n\nNode.update_forward_refs()\nDirectoryTree.update_forward_refs()\n</code></pre> <p>The <code>Node</code> class represents a single node in the directory tree. It has a name, a list of children nodes (applicable only to folders), and a node type (either a file or a folder). The <code>print_paths</code> method can be used to print the path of the node and its children.</p> <p>The <code>DirectoryTree</code> class represents the entire directory tree. It has a single attribute, <code>root</code>, which is the root node of the tree. The <code>print_paths</code> method can be used to print the paths of the root node and its children.</p>"},{"location":"examples/recursive/#parsing-the-tree","title":"Parsing the Tree","text":"<p>We define a function <code>parse_tree_to_filesystem</code> to convert a string representing a directory tree into a filesystem structure using OpenAI.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\ndef parse_tree_to_filesystem(data: str) -&gt; DirectoryTree:\n    \"\"\"\n    Convert a string representing a directory tree into a filesystem structure\n    using OpenAI's GPT-3 model.\n\n    Args:\n        data (str): The string to convert into a filesystem.\n\n    Returns:\n        DirectoryTree: The directory tree representing the filesystem.\n    \"\"\"\n\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=DirectoryTree,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a perfect file system parsing algorithm. You are given a string representing a directory tree. You must return the correct filesystem structure.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Consider the data below:\\n{data} and return the correctly labeled filesystem\",\n            },\n        ],\n        max_tokens=1000,\n    )\n</code></pre> <p>The <code>parse_tree_to_filesystem</code> function takes a string <code>data</code> representing the directory tree and returns a <code>DirectoryTree</code> object representing the filesystem structure. It uses the OpenAI Chat API to complete the prompt and extract the directory tree.</p>"},{"location":"examples/recursive/#example-usage","title":"Example Usage","text":"<p>Let's demonstrate how to use the <code>parse_tree_to_filesystem</code></p> <p>function with an example:</p> <pre><code>root = parse_tree_to_filesystem(\n    \"\"\"\n    root\n    \u251c\u2500\u2500 folder1\n    \u2502   \u251c\u2500\u2500 file1.txt\n    \u2502   \u2514\u2500\u2500 file2.txt\n    \u2514\u2500\u2500 folder2\n        \u251c\u2500\u2500 file3.txt\n        \u2514\u2500\u2500 subfolder1\n            \u2514\u2500\u2500 file4.txt\n    \"\"\"\n)\nroot.print_paths()\n</code></pre> <p>In this example, we call <code>parse_tree_to_filesystem</code> with a string representing a directory tree.</p> <p>After parsing the string into a <code>DirectoryTree</code> object, we call <code>root.print_paths()</code> to print the paths of the root node and its children. The output of this example will be:</p> <pre><code>root                               NodeType.FOLDER\nroot/folder1                       NodeType.FOLDER\nroot/folder1/file1.txt             NodeType.FILE\nroot/folder1/file2.txt             NodeType.FILE\nroot/folder2                       NodeType.FOLDER\nroot/folder2/file3.txt             NodeType.FILE\nroot/folder2/subfolder1            NodeType.FOLDER\nroot/folder2/subfolder1/file4.txt  NodeType.FILE\n</code></pre> <p>This demonstrates how to use OpenAI's GPT-3 model to parse a string representing a directory tree and obtain the correct filesystem structure.</p> <p>I hope this example helps you understand how to leverage OpenAI Function Call for parsing recursive trees. If you have any further questions, feel free to ask!</p>"},{"location":"examples/search/","title":"Example: Segmenting Search Queries","text":"<p>In this example, we will demonstrate how to leverage the <code>MultiTask</code> and <code>enum.Enum</code> features of OpenAI Function Call to segment search queries. We will define the necessary structures using Pydantic and demonstrate how segment queries into multiple sub queries and execute them in parallel with <code>asyncio</code>.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use OpenAI Function Call to segment search queries and execute them in parallel.</p>"},{"location":"examples/search/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's model the problem as breaking down a search request into a list of search queries. We will use an enum to represent different types of searches and take advantage of Python objects to add additional query logic.</p> <pre><code>import enum\nfrom pydantic import Field\n\nclass SearchType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of searches that can be performed.\"\"\"\n    VIDEO = \"video\"\n    EMAIL = \"email\"\n\nclass Search(BaseModel):\n    \"\"\"\n    Class representing a single search query.\n    \"\"\"\n    title: str = Field(..., description=\"Title of the request\")\n    query: str = Field(..., description=\"Query to search for relevant content\")\n    type: SearchType = Field(..., description=\"Type of search\")\n\n    async def execute(self):\n        print(f\"Searching for `{self.title}` with query `{self.query}` using `{self.type}`\")\n</code></pre> <p>Notice that we have added the <code>execute</code> method to the <code>Search</code> class. This method can be used to route the search query based on the enum type. You can add logic specific to each search type in the <code>execute</code> method.</p> <p>Next, let's define a class to represent multiple search queries.</p> <pre><code>from typing import List\n\nclass MultiSearch(BaseModel):\n    \"Correctly segmented set of search results\"\n    tasks: List[Search]\n</code></pre> <p>The <code>MultiSearch</code> class has a single attribute, <code>tasks</code>, which is a list of <code>Search</code> objects.</p> <p>This pattern is so common that we've added a helper function <code>MultiTask</code> to makes this simpler</p> <pre><code>from instructor.dsl import MultiTask\n\nMultiSearch = MultiTask(Search)\n</code></pre>"},{"location":"examples/search/#calling-completions","title":"Calling Completions","text":"<p>To segment a search query, we will use the base openai api. We can define a function that takes a string and returns segmented search queries using the <code>MultiSearch</code> class.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\ndef segment(data: str) -&gt; MultiSearch:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0.1,\n        functions=[MultiSearch.openai_schema],\n        function_call={\"name\": MultiSearch.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Consider the data below: '\\n{data}' and segment it into multiple search queries\",\n            },\n        ],\n        max_tokens=1000,\n    )\n</code></pre> <p>The <code>segment</code> function takes a string <code>data</code> and creates a completion. It prompts the model to segment the data into multiple search queries and returns the result as a <code>MultiSearch</code> object.</p>"},{"location":"examples/search/#evaluating-an-example","title":"Evaluating an Example","text":"<p>Let's evaluate an example by segmenting a search query and executing the segmented queries.</p> <pre><code>import asyncio\n\nqueries = segment(\"Please send me the video from last week about the investment case study and also documents about your GDPR policy?\")\n\nasync def execute_queries(queries: Multisearch):\n    await asyncio.gather(*[q.execute() for q in queries.tasks])\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(execute_queries())\nloop.close()\n</code></pre> <p>In this example, we use the <code>segment</code> function to segment the search query. We then use <code>asyncio</code> to asynchronously execute the queries using the <code>execute</code> method defined in the <code>Search</code> class.</p> <p>The output will be:</p> <pre><code>Searching for `Please send me the video from last week about the investment case study` with query `Please send me the video from last week about the investment case study` using `SearchType.VIDEO`\nSearching for `also documents about your GDPR policy?` with query `also documents about your GDPR policy?` using `SearchType.EMAIL`\n</code></pre>"},{"location":"examples/self_critique/","title":"Self-Correction with <code>llm_validator</code>","text":""},{"location":"examples/self_critique/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to use <code>llm_validator</code> for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages.</p>"},{"location":"examples/self_critique/#setup","title":"Setup","text":"<p>Import required modules and apply compatibility patches.</p> <pre><code>from typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\n</code></pre>"},{"location":"examples/self_critique/#defining-models","title":"Defining Models","text":"<p>Before building validation logic, define a basic Pydantic model named <code>QuestionAnswer</code>. We'll use this model to generate a response without validation to see the output.</p> <pre><code>class QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n</code></pre>"},{"location":"examples/self_critique/#generating-a-response","title":"Generating a Response","text":"<p>Here we coerce the model to generate a response that is objectionable.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\"\n\nqa: QuestionAnswer = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#output-before-validation","title":"Output Before Validation","text":"<p>While it calls out the objectionable content, it doesn't provide any details on how to correct it.</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\"\n}\n</code></pre>"},{"location":"examples/self_critique/#adding-custom-validation","title":"Adding Custom Validation","text":"<p>By adding a validator to the <code>answer</code> field, we can try to catch the issue and correct it. Lets integrate <code>llm_validator</code> into the model and see the error message. Its important to note that you can use all of pydantic's validators as you would normally as long as you raise a <code>ValidationError</code> with a helpful error message as it will be used as part of the self correction prompt.</p> <pre><code>class QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", allow_override=True)\n        ),\n    ]\n\ntry:\n    qa: QuestionAnswerNoEvil = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=QuestionAnswerNoEvil,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n            },\n        ],\n    )\nexcept Exception as e:\n    print(e)\n</code></pre>"},{"location":"examples/self_critique/#output-after-validation","title":"Output After Validation","text":"<p>Now, we throw validation error that its objectionable and provide a helpful error message.</p> <pre><code>1 validation error for QuestionAnswerNoEvil\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which is objectionable.\n</code></pre>"},{"location":"examples/self_critique/#retrying-with-corrections","title":"Retrying with Corrections","text":"<p>By adding the <code>max_retries</code> parameter, we can retry the request with corrections. and use the error message to correct the output.</p> <pre><code>qa: QuestionAnswerNoEvil = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswerNoEvil,\n    max_retries=1,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#final-output","title":"Final Output","text":"<p>Now, we get a valid response that is not objectionable!</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\"\n}\n</code></pre>"},{"location":"blog/archive/2023/","title":"2023","text":""}]}